%%%%%%%%  Document class  %%%%%%%%%%%%
\documentclass[10pt, twoside, a4paper]{book}

%%%%%%%%  Packages   %%%%%%%%%%%%%%%%
\usepackage{natbib}
%\usepackage{d:/laurim/biometria/lshort/src/lshort}
%\usepackage{numline}
%\usepackage{lineno}
\bibpunct{(}{)}{,}{a}{}{,}
\usepackage{amsmath,amssymb}
\usepackage{bm} 
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{color}

%\newenvironment{Rcode}[1]%
%{\scriptsize{\ttfamily{\bf The R-code for #1}} \\
%\ttfamily \noindent\ignorespaces }
%{\par\noindent%
%  \ignorespacesafterend}
  
\newenvironment{Rcode}[1]%
{\ttfamily{\bf The R-code for #1}
\scriptsize \\\par\verbatim}
{\endverbatim\par} 

\newenvironment{Rcode2}%
{\scriptsize \par\verbatim}%
{\endverbatim\par}

\usepackage{graphicx}
%\usepackage{subfigure}
%\usepackage[nolists]{endfloat}
\usepackage{amsthm}

\usepackage{times}
%\usepackage[T1]{fontenc}

%\usepackage{breqn} % Helps with multline \left \right problems.

\newcommand{\foldPath}{c:/laurim/tex/}

\newcommand{\beq}{\begin{equation*}}
\newcommand{\eeq}{\end{equation*}}
\newcommand{\ud}{\,\mathrm{d}}
\newcommand{\ureal}{\,\mathbb{R}}
\newcommand{\uninf}{\,-\infty}
\newcommand{\uinf}{\,-\infty}
\usepackage{graphicx}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\corr}{corr}
\DeclareMathOperator{\sd}{sd}

\theoremstyle{definition}

\newtheorem{example}{Example}[chapter]
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
%\bibpunct{(}{)}{,}{a}{}{,}
\bibliographystyle{\foldPath saf}
%\bibliographystyle{alpha}
%\bibliographystyle{\foldPath wiley}
%\bibliographystyle{unsrt}

%%%%%%%%  Page Setup %%%%%%%%%%%%%%%%%%
%\topmargin      0pt
%\headheight     0pt 
%\headsep        0pt 
%\textheight   648pt 
%\oddsidemargin  0pt
%\textwidth    468pt 

%%%%%%%% Document %%%%%%%%%%%%%%%%%%%
\begin{document}
\pagenumbering{Roman}
\setlength{\baselineskip}{16pt}

%% Front matter
%\title{Forest biometrics with examples in R}
\title{Introduction to statistical inference 2}

\author{Lauri Meht\"atalo\\
	   University of Eastern Finland \\ School of Computing
	   }

\date{\normalsize \today}
\maketitle
\tableofcontents

%\section*{Preface}

%These are lecture notes for course Biometrics: R-statistics in Forest Sciences. In writing these notes, I have used several sources. In writing chapter 1, the main sources have been the second edition of the classical book ``Statistical Inference'' by George Casella and Roger L Berger\citep{Casellaandberger2002} and ``Methods for Forest Biometrics'' by Juha Lappi  \citep{Lappi1993}. In addition, I have used lecture notes of courses on mathematical statistics (by Eero Korpelainen) and statistical inference (by Jukka Nyblom). For chapters 2 and 3, the main sources have been the lecture notes of Jukka Nyblom on Regression analysis and Linear models, the book of Jose Pinheiro and Douglas Bates on Linear models with R \citep{Pinheiroandbates2000}, and the book ``Generalized, Linear and Mixed Models'' by CharlesMcCulloch and Shayle R Searle \citep{McCullochandsearle2001}. For the GLMs and GLMMs, I have used \citet{McCullochandsearle2001}, too. The last chapter on models systems is based mainly on book ``Econometric analysis'' by William H Greene \citep{Greene1997}. In addition, good sources of information have been the lecture notes of Annika Kangas on Forest Biometrics, the books of of Julian Faraway \citep{Faraway2004, Faraway2006}, Keith E Muller and Paul W Stewart \citep{Mullerandstewart2006}, and William N. Venables and Brian D. Ripley \citep{Venablesandripley2002}.   

\mainmatter
\pagenumbering{arabic}

\chapter {Recap from ``Introduction to statistical inference 1''}
\section{Random variable}
\begin{itemize}
\item Random variable is a function from sample space $\mathcal{S}$ of an
experiment to sample space of the random variable $\mathcal{X}$, which is set of real numbers.
$$X: \mathcal{S} \rightarrow \mathcal{X}$$
\item The sample space is a set of all possible values random variable can get.
\item $\mathcal{X}$ can be
\begin{itemize}
  \item an interval of real axis (continuous random variable).
  $$\mathcal{X} = [0, 10),~ \mathcal{X} = [0, 10],~ \mathcal{X} = (0, 10)$$
  \item An uncountable set of integers (discrete random variable) 
  $$\mathcal{X} = \{0,1,2,\ldots\}$$
  \item A countable set of integers or real numbers (discrete random variable)
  $$\mathcal{X} =\{0,1\},~\mathcal{X} = \{0,0.5,1\},~ \mathcal{X} =
  \{0,1,\ldots,10\}$$ 
\end{itemize}
\item Probabilities associated with each value of $X$ are defined by the
cumulative distribution function (cdf for short).
$$F_X(x) = P(X \leq x),~\text{where } -\infty < x < \infty$$
\item[\bf{Note:}]$F_X(x)$ is a step function if $X$ is discrete.
\item[\bf{Note:}]$F_X(x)$ is a continuous function if $X$ is continuous.
%TODO: Two plots visualizing discreate and continous CDFs. Notes 1, page 2.
\item $F_X(x)$ or $F(x)$ is cdf, if
\begin{itemize}
  \item [1)] $\lim_{x \to -\infty} F(x) = 0$ and $\lim_{x \to
  \infty} F(x) = 1$
  \item [2)] $F(x)$ is non-decreasing
  \item [3)] $F(x)$ is right-continuous
\end{itemize}
\item Cdf is useful in calculation of any probabilities; for example
$$P(a < X \leq b) = F(b) - F(a)$$
\item [\bf{Note:}] Be careful with $<$ and $\leq$ when working with discreate
random variables.
\item The probability density function (pdf for short) is defined for
continuous random variable as
$$f_X(x) = F'(x) = \frac{\ud F_X(x)}{\ud x}, ~ -\infty < x < \infty$$
and
$$\int_{-\infty}^x f_X(t)\ud t = F_X(x)$$
\item The probability mass function (pmf for short) is defined for discrete
random variables as
$$f_X(x) = P(X=x)$$
$$F_X(x) = \sum_{k=1}^x f_X(k)$$
\end{itemize}
\section{Transformations of random variable}
\begin{itemize}
  \item Consider a monotonic function $g: \mathcal{X} \to \mathcal{Y}$
  \item $Y = g(X)$ is also a random variable; function $g$ is called an
  transformation (muunnos).
  \item If $g(x)$ is a increasing function of $x$, then
  $$F_Y(y) = F_X(g^{-1}(y))$$
  \item If $g(x)$ is a decreasing function of $x$, then
  $$F_Y(y) = 1 - F_X(g^{-1}(y))$$
  \item The pdf of continuous $Y$ is
  $$f_Y(y) = F_Y'(y)$$
\end{itemize}
\section{Expected values}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
E(X) = \mu_X = \left\{
\begin{array}{ll}
\int_{-\infty}^\infty x f(x) \ud x & \text{if $X$ is continuous} \\
\sum_{x \in \mathcal{X}} x f(x) & \text{if $X$ is discrete}
\end{array} \right.
\end{equation*}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
E(g(X)) =  \left\{
\begin{array}{ll}
\int_{-\infty}^\infty g(x) f(x) \ud x & \text{if $X$ is continuous} \\
\sum_{x \in \mathcal{X}} g(x) f(x) & \text{if $X$ is discrete}
\end{array} \right.
\end{equation*}
\section{Variance}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
\sigma_X^2 = Var(X) & = E(X - \mu_X)^2 \\
& = E(X^2 - 2X\mu_X + \mu_X^2) \\
& = E(X^2) - E(2X\mu_X) + E(\mu_X^2) \\
& = E(X^2) - \underbrace{2\mu_X \underbrace{E(X)}_{\mu_X}}_{2\mu_X^2} +
E(\mu_X^2) \\
& = E(X^2) - \mu_X^2 \\
\end{array}
\end{equation*}

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
sd(X) = \sqrt{Var(X)} = \sigma_X
\end{array}
\end{equation*}

\section{Bivariate random variables}
\begin{itemize}
  \item For two discrete random variables, the joint pmf is defined as
  $$f_{X,Y}(x,y)=P(X=x, Y=y)$$
  \item For two continuous random variables, we define the joint pdf
  $f_{X,Y}(x,y)$ as
  $$P((X,Y) \in A) = \iint\limits_A f(x,y) \ud x \ud y$$
  \item The expected value for transformation
$g(X,Y) : \ureal^2 \to \ureal$ (for example, \\ $g(X,Y) = XY$ or
$g(X,Y) =
\frac{X}{Y}$) is
$$E(g(X,Y)) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y)f(x,y) \ud
x \ud y$$
if $(X,Y)$ is continuous, and
$$E(g(X,Y)) = \sum_{x,y \in \ureal^2} g(x,y)f(x,y)$$
if $(X,Y)$ is discrete.
\item The marginal pmf / pdf for $X$ are
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
f_X(x) = \sum_{y \in \ureal} f_{X,Y}(x,y) & \text{(pmf)} \\
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \ud y & \text{(pdf)} \\
\end{array}
\end{equation*}
and correspondingly for $Y$.
\item The conditional pmf / pdf are both defined as
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
f(y \mid x)  = \frac{f_{X,Y}(x,y)}{f_X(x)} & \text{(for both discrete and
continuous random variables)}
\end{array}
\end{equation*}
and correspondingly for $x \mid y$.
\end{itemize}
\section{Independence}
\begin{itemize}
  \item Random variables are said to be independent ($X \indep Y$) if
  $$f_{X,Y}(x,y) = f_X(x)f_Y(y)$$
  \item For independent random variables, conditional distribution $y \mid x$ is 
  $$f(y \mid x) = f_y(y)$$
  regardless of the value of $x$.
\end{itemize}

\section{Covariance}
\begin{itemize}
  \item Covariance measures the linear association between two random variables
  $X$ and $Y$
  \begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{lll}
	cov(X,Y) & = E((X-\mu_X)(Y-\mu_Y)) \\
	& = E(XY) - \mu_X\mu_Y \\
	cov(X,Y) & = cov(Y,X) \\
	corr(X,Y) & = \rho_{XY} = \frac{cov(X,Y)}{\sigma_X \sigma_Y} & \text{where } -1
	\leq \rho_{XY} \leq 1
	\end{array}
	\end{equation*}
	\item[\bf{Note:}] $cov(X,X) = Var(X)$
	%TODO: Small plot left out. Notes 1, page 8.
	\item[\bf{Note:}] If $X \indep Y$, then $cov(X,Y) = 0$, but if $cov(X,Y) = 0$,
	it does not mean $X$ and $Y$ are necessarily independent.
\end{itemize}
\section{Random vectors}
\begin{itemize}
  \item Random vectors generalize the bivariate random variables to a
  $n$-variate case.
  	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	\bm X = \left [
	\begin{matrix}
	X_1 \\ X_2 \\ \vdots \\ X_n
	\end{matrix} \right ] & \text{where $X_i,~i=1,2,\ldots,n$ are scalar random
	variables}
	\end{array}
	\end{equation*}
	\item If $\bm x$ is a continuous random vector, then
	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	P(\bm X \in A) = \idotsint\limits_A f(\bm x) \ud x_1 \ldots \ud x_n &
	f: \ureal^2 \to \ureal
	\end{array}
	\end{equation*}
	where $f(\bm x)$ is a joint pdf.
	\item If $\bm x$ is a discrete random vector, then
	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	P(\bm X \in A) = \sum \ldots \sum f(\bm x)
	\end{array}
	\end{equation*}
	where $f(\bm x)$ is a joint pmf.

\item Let $g(\bm x)$ be a transformation $g: \ureal^2 \to \ureal$. Then, the
	expected value is
	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	E(g(\bm X)) = \left\{
	\begin{array}{ll}
	\int_{-\infty}^\infty \ldots \int_{-\infty}^\infty x f(x) \ud x_1 \ldots \ud
	x_n & \text{if $\bm X$ is continuous} \\
	\mathop{\sum\ldots\sum}_{\bm X \in \ureal^2}g(\bm x)f(\bm x)& \text{if $\bm X$
	is discrete} \end{array} \right.
	\end{equation*}
	\item Let us partition the $n$-variate random vector $\bm X$ as follows:
	$$\bm X = \left(\begin{matrix}\bm X_1 \\ \bm X_2\end{matrix}\right)$$
	where $\bm X_1$ has lenght $k$ and $\bm X_2$ has length $n-k$. 
	\item The joint pdf
	of $\bm X$ can be written as
	$$f(\bm x) = f(\bm x_1, \bm x_2)$$ 
	%TODO: In notes, X_1 and X_2 normal random variables. Is this correct?
	\item The marginal density of $\bm X_1$ is
	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	f(\bm x_1) = \int\limits_{\ureal^{n-k}} f(\bm x_1, \bm x_2) \ud \bm x_2 &
	\text{if $\bm X$ is continuous} \\
	f(\bm x_1) = \sum\limits_{\ureal^{n-k}} f(\bm x_1, \bm x_2) &
	\text{if $\bm X$ is discrete}
	\end{array}
	\end{equation*}
	where $f(\bm x_1)$ is a $k$-variate pdf/pmf.
	
	%TODO: Left out notes 2 page 2 stuff about pdf and pmf sample spaces and plots,
	% since they are little out of context in notes. Are they needed?
	
	\item The conditional pdf/pmf for $\bm X_2 \mid \bm X_1$ is
	$$f(\bm X_2 \mid \bm X_1) = \frac{f(\bm X_1, \bm X_2)}{f(\bm X_2)}$$
	
	%TODO: Left out plots and table visualization from notes 2 page 2 (end), are
	% they needed?
	
	\item Expected value of random vectors is defined as vector
	
	\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}

E(\bm x) = 
\begin{bmatrix}
E(X_1) \\ E(X_2) \\ \vdots \\ E(X_n)
\end{bmatrix}_{n \times 1} = 
\begin{bmatrix}
E(\bm X_1) \\ E(\bm X_2)
\end{bmatrix} =
\begin{bmatrix}
\bm \mu_1 \\ \bm \mu_2 \\
\end{bmatrix}
\end{array}
\end{equation*}
	
	\item The variance of a random vector is $n \times n$ symmetric matrix called
variance-covariance matrix.

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}

Var(\bm x)_{n \times n} & = 
\begin{bmatrix}
Var(X_1) & cov(X_1, X_2) & cov(X_1, X_3) & \hdots & cov(X_1, X_n) \\
cov(X_2, X_1) & Var(X_2) & cov(X_2, X_3) & \hdots & cov(X_2, X_n) \\
cov(X_3, X_1) & cov(X_3, X_2) & Var(X_3) & \hdots & cov(X_3, X_n) \\
\vdots & \vdots & \vdots & \ddots  & \vdots \\
cov(X_n, X_1) & cov(X_n, X_2) & cov(X_n, X_3) & \hdots & Var(X_n) \\
\end{bmatrix} \\ \\
 & = 
 
\begin{bmatrix}
Var(\bm x_1)_{k \times k} & cov(\bm x_1, \bm x_2')_{k \times (n-k)} \\ 
cov(\bm x_2, \bm x_1')_{(n-k) \times k}& Var(\bm x_2)_{(n-k) \times (n-k)}
\end{bmatrix}
\end{array}
\end{equation*}

\item The correlation matrix is defined as

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}

corr(\bm x)_{n \times n} & = 
\begin{bmatrix}
1 & corr(X_1, X_2) & corr(X_1, X_3) & \hdots & corr(X_1, X_n) \\
corr(X_2, X_1) & 1 & corr(X_2, X_3) & \hdots & corr(X_2, X_n) \\
corr(X_3, X_1) & corr(X_3, X_2) & 1 & \hdots & corr(X_3, X_n) \\
\vdots & \vdots & \vdots & \ddots  & \vdots \\
corr(X_n, X_1) & corr(X_n, X_2) & corr(X_n, X_3) & \hdots & 1 \\
\end{bmatrix}
\end{array}
\end{equation*}

\item If $\bm X$ has a $n$-variate normal distribution, then, with
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{lll}

E(\bm x) = 
\bm \mu =
\begin{bmatrix}
\bm \mu_1 \\ \bm \mu_2 \\
\end{bmatrix}

 & \text{ and } & 
 
 Var(\bm x)_{n \times n} = 
\bm \Sigma =
\begin{bmatrix}
\bm \Sigma_1 & \bm \Sigma_{12} \\
\bm \Sigma_{21} & \bm \Sigma_2
\end{bmatrix}

\end{array}
\end{equation*}
then $\bm X_1 \mid \bm X_2$ has a $k$-variate normal distribution with
$$E(\bm X_1 \mid \bm X_2) = \bm \mu_1 + \bm \Sigma_{12} \bm
\Sigma_2^{-1}(\bm X_2 - \bm \mu_2)$$
$$Var(\bm X_1 \mid \bm X_2) = \bm \Sigma_1 - \bm \Sigma_{12} \bm \Sigma_2^{-1}
\bm \Sigma_{21}$$
\item[\bf{Note:}] $Var(\bm X_1 \mid \bm X_2) \leq Var(\bm X_1)$

\end{itemize}

\section{Computing using expected values and variances}
Let $a$, $b$ and $c$ be constants, and let $X$, $Y$ and $Z$ be (scalar) random
variables. The following rules hold regardless of the distribution of random
variables $X$, $Y$ and $Z$.

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}

E(c) = c \\
E(cX) = cE(X) \\
E(X+Y) = E(X)+E(Y) \\
E(X + c) = E(X)+c \\
E(XY) = E(X)E(Y) & \text{Only if $X \indep Y$.} \\
E(g(X)) = g(E(X)) & \text{Only in some special cases, like when $g(X)$ }\\
&\text{is a linear transformation.} \\
Var(X+Y) = Var(X)+Var(Y)+2cov(X,Y) \\
Var(X-Y) = Var(X)+Var(Y)-2cov(X,Y) \\
Var(aX) = a^2 \cdot Var(X) \\
cov(X, Y+Z) = cov(X,Y)+cov(X,Z) \\
cov(aX,bY) = ab\cdot cov(XY) \\
Var(X+a) = Var(X) \\
%TODO: Plot in notes 2 page 5 left out
cov(X+a,Y+b) = cov(X,Y) \\
E(X) = E_Y\left[E_{X\mid Y}(X \mid Y)\right] \\
Var(X) = E_Y\left[Var_{X\mid Y}(X \mid Y)\right]+Var_Y\left[Var_{X\mid Y}(X \mid
Y)\right]
\end{array}
\end{equation*}
Let $\bm a$ and $\bm b$ be fixed vectors and $\bm X$ and $\bm Y$ random vectors
so that the dimensions in the equations match.
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
E(\bm a' \bm X) = \bm a' E(\bm X) \\
Var(\bm a' \bm X) = \bm a' Var(\bm X) \bm a & \text{Compare to }Var(aX) = a^2
\cdot Var(X) = a \cdot Var(X) \cdot a \\
cov(\bm a' \bm X, \bm b' \bm Y) = \bm a' cov(\bm X, \bm Y) \bm b
\end{array}
\end{equation*}
{\bf Note: } These equations need to be remembered by hearth!

\chapter{Random samples}
\begin{definition}
\label{defRandomSamples1}
Random variables $X_1,..,X_n$ are called random sample of size $n$ from
population $f(x)$, if $X_1,\ldots,X_n$ are mutually independent random variables
and the marginal pdf/pmf if each $X_i$ is the same function $f(x)$.
Alternatively, $X_1,\ldots,X_n$ are called independent and identically
distributed random variables (i.i.d.) with pdf/pmf $f(x)$.

\textbf{Note:} Sample $X_1,\ldots,X_n$ can also be denoted by $\bm X$, where
$\bm X = \begin{bmatrix}X_1 & \hdots & X_n \end{bmatrix}^T$
\begin{itemize}
\item If follows from the mutual independence, of $X_1,\ldots,X_n$ that the
joint pdf or pmf of $\bm X$ is
$$f(x_1,\ldots,x_2) = f(x_1)f(x_2)\ldots f(x_n) = \prod_{i=1}^n f(x_i)$$
\item All univariate marginal distributions $f(x_i)$ are the same by definition
\ref{defRandomSamples1}.
\end{itemize}
\begin{example}
Let $X_1,\ldots,X_n$ be a random sample from $Exponential(\beta)$ population.
$X_i$ specifies the time until failure for $n$ identical cellphones. The
exponential pdf is
$$f(x_i) = \frac{1}{\beta}e^{-x_i/\beta}$$ %TODO: Is the x_i correct?(note 2 p6)
The joint pdf pf the sample is
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
f(x_1, x_2, \ldots, x_n \mid \beta) = \prod_{i=1}^n
\frac{1}{\beta}e^{-x_i/\beta} = \frac{1}{\beta^n}e^{\frac{1}{\beta}\sum x_i} &
\text{Recall: } a^ba^c=a^{b+c}
\end{array}
\end{equation*}
What is the probability that all $n$ cellphones last more than 2 years?
\end{example}
\end{definition}
\end{document}
