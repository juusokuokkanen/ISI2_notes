%%%%%%%%  Document class  %%%%%%%%%%%%
\documentclass[10pt, twoside, a4paper]{book}

%%%%%%%%  Packages   %%%%%%%%%%%%%%%%
\usepackage{natbib}
%\usepackage{d:/laurim/biometria/lshort/src/lshort}
%\usepackage{numline}
%\usepackage{lineno}
\bibpunct{(}{)}{,}{a}{}{,}
\usepackage{amsmath,amssymb}
\usepackage{bm} 
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{color}

%\newenvironment{Rcode}[1]%
%{\scriptsize{\ttfamily{\bf The R-code for #1}} \\
%\ttfamily \noindent\ignorespaces }
%{\par\noindent%
%  \ignorespacesafterend}
  
\newenvironment{Rcode}[1]%
{\ttfamily{\bf The R-code for #1}
\scriptsize \\\par\verbatim}
{\endverbatim\par} 

\newenvironment{Rcode2}%
{\scriptsize \par\verbatim}%
{\endverbatim\par}

\usepackage{graphicx}
%\usepackage{subfigure}
%\usepackage[nolists]{endfloat}
\usepackage{amsthm}

\usepackage{times}
%\usepackage[T1]{fontenc}

%\usepackage{breqn} % Helps with multline \left \right problems.

\newcommand{\foldPath}{c:/laurim/tex/}

\newcommand{\beq}{\begin{equation*}}
\newcommand{\eeq}{\end{equation*}}
\newcommand{\ud}{\,\mathrm{d}}
\newcommand{\ureal}{\,\mathbb{R}}
\newcommand{\uninf}{\,-\infty}
\newcommand{\uinf}{\,-\infty}
\usepackage{graphicx}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\corr}{corr}
\DeclareMathOperator{\sd}{sd}

\theoremstyle{definition}

\newtheorem{example}{Example}[chapter]
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
%\bibpunct{(}{)}{,}{a}{}{,}
\bibliographystyle{\foldPath saf}
%\bibliographystyle{alpha}
%\bibliographystyle{\foldPath wiley}
%\bibliographystyle{unsrt}

%%%%%%%%  Page Setup %%%%%%%%%%%%%%%%%%
%\topmargin      0pt
%\headheight     0pt 
%\headsep        0pt 
%\textheight   648pt 
%\oddsidemargin  0pt
%\textwidth    468pt 

%%%%%%%% Document %%%%%%%%%%%%%%%%%%%
\begin{document}
\pagenumbering{Roman}
\setlength{\baselineskip}{16pt}

%% Front matter
%\title{Forest biometrics with examples in R}
\title{Introduction to statistical inference 2}

\author{Lauri Meht\"atalo\\
	   University of Eastern Finland \\ School of Computing
	   }

\date{\normalsize \today}
\maketitle
\tableofcontents

%\section*{Preface}

%These are lecture notes for course Biometrics: R-statistics in Forest Sciences. In writing these notes, I have used several sources. In writing chapter 1, the main sources have been the second edition of the classical book ``Statistical Inference'' by George Casella and Roger L Berger\citep{Casellaandberger2002} and ``Methods for Forest Biometrics'' by Juha Lappi  \citep{Lappi1993}. In addition, I have used lecture notes of courses on mathematical statistics (by Eero Korpelainen) and statistical inference (by Jukka Nyblom). For chapters 2 and 3, the main sources have been the lecture notes of Jukka Nyblom on Regression analysis and Linear models, the book of Jose Pinheiro and Douglas Bates on Linear models with R \citep{Pinheiroandbates2000}, and the book ``Generalized, Linear and Mixed Models'' by CharlesMcCulloch and Shayle R Searle \citep{McCullochandsearle2001}. For the GLMs and GLMMs, I have used \citet{McCullochandsearle2001}, too. The last chapter on models systems is based mainly on book ``Econometric analysis'' by William H Greene \citep{Greene1997}. In addition, good sources of information have been the lecture notes of Annika Kangas on Forest Biometrics, the books of of Julian Faraway \citep{Faraway2004, Faraway2006}, Keith E Muller and Paul W Stewart \citep{Mullerandstewart2006}, and William N. Venables and Brian D. Ripley \citep{Venablesandripley2002}.   

\mainmatter
\pagenumbering{arabic}

\chapter {Recap from ``Introduction to statistical inference 1''}
\section{Random variable}
\begin{itemize}
\item Random variable is a function from sample space $\mathcal{S}$ of an
experiment to sample space of the random variable $\mathcal{X}$, which is set of real numbers.
$$X: \mathcal{S} \rightarrow \mathcal{X}$$
\item The sample space is a set of all possible values random variable can get.
\item $\mathcal{X}$ can be
\begin{itemize}
  \item an interval of real axis (continuous random variable).
  $$\mathcal{X} = [0, 10),~ \mathcal{X} = [0, 10],~ \mathcal{X} = (0, 10)$$
  \item An uncountable set of integers (discrete random variable) 
  $$\mathcal{X} = \{0,1,2,\ldots\}$$
  \item A countable set of integers or real numbers (discrete random variable)
  $$\mathcal{X} =\{0,1\},~\mathcal{X} = \{0,0.5,1\},~ \mathcal{X} =
  \{0,1,\ldots,10\}$$ 
\end{itemize}
\item Probabilities associated with each value of $X$ are defined by the
cumulative distribution function (cdf for short).
$$F_X(x) = P(X \leq x),~\text{where } -\infty < x < \infty$$
\item[\bf{Note:}]$F_X(x)$ is a step function if $X$ is discrete.
\item[\bf{Note:}]$F_X(x)$ is a continuous function if $X$ is continuous.
%TODO: Two plots visualizing discreate and continous CDFs. Notes 1, page 2.
\item $F_X(x)$ or $F(x)$ is cdf, if
\begin{itemize}
  \item [1)] $\lim_{x \to -\infty} F(x) = 0$ and $\lim_{x \to
  \infty} F(x) = 1$
  \item [2)] $F(x)$ is non-decreasing
  \item [3)] $F(x)$ is right-continuous
\end{itemize}
\item Cdf is useful in calculation of any probabilities; for example
$$P(a < X \leq b) = F(b) - F(a)$$
\item [\bf{Note:}] Be careful with $<$ and $\leq$ when working with discreate
random variables.
\item The probability density function (pdf for short) is defined for
continuous random variable as
$$f_X(x) = F'(x) = \frac{\ud F_X(x)}{\ud x}, ~ -\infty < x < \infty$$
and
$$\int_{-\infty}^x f_X(t)\ud t = F_X(x)$$
\item The probability mass function (pmf for short) is defined for discrete
random variables as
$$f_X(x) = P(X=x)$$
$$F_X(x) = \sum_{k=1}^x f_X(k)$$
\end{itemize}
\section{Transformations of random variable}
\begin{itemize}
  \item Consider a monotonic function $g: \mathcal{X} \to \mathcal{Y}$
  \item $Y = g(X)$ is also a random variable; function $g$ is called an
  transformation (muunnos).
  \item If $g(x)$ is a increasing function of $x$, then
  $$F_Y(y) = F_X(g^{-1}(y))$$
  \item If $g(x)$ is a decreasing function of $x$, then
  $$F_Y(y) = 1 - F_X(g^{-1}(y))$$
  \item The pdf of continuous $Y$ is
  $$f_Y(y) = F_Y'(y)$$
\end{itemize}
\section{Expected values}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
E(X) = \mu_X = \left\{
\begin{array}{ll}
\int_{-\infty}^\infty x f(x) \ud x & \text{if $X$ is continuous} \\
\sum_{x \in \mathcal{X}} x f(x) & \text{if $X$ is discrete}
\end{array} \right.
\end{equation*}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
E(g(X)) =  \left\{
\begin{array}{ll}
\int_{-\infty}^\infty g(x) f(x) \ud x & \text{if $X$ is continuous} \\
\sum_{x \in \mathcal{X}} g(x) f(x) & \text{if $X$ is discrete}
\end{array} \right.
\end{equation*}
\section{Variance}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
\sigma_X^2 = Var(X) & = E(X - \mu_X)^2 \\
& = E(X^2 - 2X\mu_X + \mu_X^2) \\
& = E(X^2) - E(2X\mu_X) + E(\mu_X^2) \\
& = E(X^2) - \underbrace{2\mu_X \underbrace{E(X)}_{\mu_X}}_{2\mu_X^2} +
E(\mu_X^2) \\
& = E(X^2) - \mu_X^2 \\
\end{array}
\end{equation*}

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
sd(X) = \sqrt{Var(X)} = \sigma_X
\end{array}
\end{equation*}

\section{Bivariate random variables}
\begin{itemize}
  \item For two discrete random variables, the joint pmf is defined as
  $$f_{X,Y}(x,y)=P(X=x, Y=y)$$
  \item For two continuous random variables, we define the joint pdf
  $f_{X,Y}(x,y)$ as
  $$P((X,Y) \in A) = \iint\limits_A f(x,y) \ud x \ud y$$
  \item The expected value for transformation
$g(X,Y) : \ureal^2 \to \ureal$ (for example, \\ $g(X,Y) = XY$ or
$g(X,Y) =
\frac{X}{Y}$) is
$$E(g(X,Y)) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y)f(x,y) \ud
x \ud y$$
if $(X,Y)$ is continuous, and
$$E(g(X,Y)) = \sum_{x,y \in \ureal^2} g(x,y)f(x,y)$$
if $(X,Y)$ is discrete.
\item The marginal pmf / pdf for $X$ are
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
f_X(x) = \sum_{y \in \ureal} f_{X,Y}(x,y) & \text{(pmf)} \\
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \ud y & \text{(pdf)} \\
\end{array}
\end{equation*}
and correspondingly for $Y$.
\item The conditional pmf / pdf are both defined as
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
f(y \mid x)  = \frac{f_{X,Y}(x,y)}{f_X(x)} & \text{(for both discrete and
continuous random variables)}
\end{array}
\end{equation*}
and correspondingly for $x \mid y$.
\end{itemize}
\section{Independence}
\begin{itemize}
  \item Random variables are said to be independent ($X \indep Y$) if
  $$f_{X,Y}(x,y) = f_X(x)f_Y(y)$$
  \item For independent random variables, conditional distribution $y \mid x$ is 
  $$f(y \mid x) = f_y(y)$$
  regardless of the value of $x$.
\end{itemize}

\section{Covariance}
\begin{itemize}
  \item Covariance measures the linear association between two random variables
  $X$ and $Y$
  \begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{lll}
	cov(X,Y) & = E((X-\mu_X)(Y-\mu_Y)) \\
	& = E(XY) - \mu_X\mu_Y \\
	cov(X,Y) & = cov(Y,X) \\
	corr(X,Y) & = \rho_{XY} = \frac{cov(X,Y)}{\sigma_X \sigma_Y} & \text{where } -1
	\leq \rho_{XY} \leq 1
	\end{array}
	\end{equation*}
	\item[\bf{Note:}] $cov(X,X) = Var(X)$
	%TODO: Small plot left out. Notes 1, page 8.
	\item[\bf{Note:}] If $X \indep Y$, then $cov(X,Y) = 0$, but if $cov(X,Y) = 0$,
	it does not mean $X$ and $Y$ are necessarily independent.
\end{itemize}
\section{Random vectors}
\begin{itemize}
  \item Random vectors generalize the bivariate random variables to a
  $n$-variate case.
  	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	\bm X = \left [
	\begin{matrix}
	X_1 \\ X_2 \\ \vdots \\ X_n
	\end{matrix} \right ] & \text{where $X_i,~i=1,2,\ldots,n$ are scalar random
	variables}
	\end{array}
	\end{equation*}
	\item If $\bm x$ is a continuous random vector, then
	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	P(\bm X \in A) = \idotsint\limits_A f(\bm x) \ud x_1 \ldots \ud x_n &
	f: \ureal^2 \to \ureal
	\end{array}
	\end{equation*}
	where $f(\bm x)$ is a joint pdf.
	\item If $\bm x$ is a discrete random vector, then
	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	P(\bm X \in A) = \sum \ldots \sum f(\bm x)
	\end{array}
	\end{equation*}
	where $f(\bm x)$ is a joint pmf.

\item Let $g(\bm x)$ be a transformation $g: \ureal^2 \to \ureal$. Then, the
	expected value is
	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	E(g(\bm X)) = \left\{
	\begin{array}{ll}
	\int_{-\infty}^\infty \ldots \int_{-\infty}^\infty x f(x) \ud x_1 \ldots \ud
	x_n & \text{if $\bm X$ is continuous} \\
	\mathop{\sum\ldots\sum}_{\bm X \in \ureal^2}g(\bm x)f(\bm x)& \text{if $\bm X$
	is discrete} \end{array} \right.
	\end{equation*}
	\item Let us partition the $n$-variate random vector $\bm X$ as follows:
	$$\bm X = \left(\begin{matrix}\bm X_1 \\ \bm X_2\end{matrix}\right)$$
	where $\bm X_1$ has lenght $k$ and $\bm X_2$ has length $n-k$. 
	\item The joint pdf
	of $\bm X$ can be written as
	$$f(\bm x) = f(\bm x_1, \bm x_2)$$ 
	%TODO: In notes, X_1 and X_2 normal random variables. Is this correct?
	\item The marginal density of $\bm X_1$ is
	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	f(\bm x_1) = \int\limits_{\ureal^{n-k}} f(\bm x_1, \bm x_2) \ud \bm x_2 &
	\text{if $\bm X$ is continuous} \\
	f(\bm x_1) = \sum\limits_{\ureal^{n-k}} f(\bm x_1, \bm x_2) &
	\text{if $\bm X$ is discrete}
	\end{array}
	\end{equation*}
	where $f(\bm x_1)$ is a $k$-variate pdf/pmf.
	
	%TODO: Left out notes 2 page 2 stuff about pdf and pmf sample spaces and plots,
	% since they are little out of context in notes. Are they needed?
	
	\item The conditional pdf/pmf for $\bm X_2 \mid \bm X_1$ is
	$$f(\bm X_2 \mid \bm X_1) = \frac{f(\bm X_1, \bm X_2)}{f(\bm X_2)}$$
	
	%TODO: Left out plots and table visualization from notes 2 page 2 (end), are
	% they needed?
	
	\item Expected value of random vectors is defined as vector
	
	\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}

E(\bm x) = 
\begin{bmatrix}
E(X_1) \\ E(X_2) \\ \vdots \\ E(X_n)
\end{bmatrix}_{n \times 1} = 
\begin{bmatrix}
E(\bm X_1) \\ E(\bm X_2)
\end{bmatrix} =
\begin{bmatrix}
\bm \mu_1 \\ \bm \mu_2 \\
\end{bmatrix}
\end{array}
\end{equation*}
	
	\item The variance of a random vector is $n \times n$ symmetric matrix called
variance-covariance matrix.

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}

Var(\bm x)_{n \times n} & = 
\begin{bmatrix}
Var(X_1) & cov(X_1, X_2) & cov(X_1, X_3) & \hdots & cov(X_1, X_n) \\
cov(X_2, X_1) & Var(X_2) & cov(X_2, X_3) & \hdots & cov(X_2, X_n) \\
cov(X_3, X_1) & cov(X_3, X_2) & Var(X_3) & \hdots & cov(X_3, X_n) \\
\vdots & \vdots & \vdots & \ddots  & \vdots \\
cov(X_n, X_1) & cov(X_n, X_2) & cov(X_n, X_3) & \hdots & Var(X_n) \\
\end{bmatrix} \\ \\
 & = 
 
\begin{bmatrix}
Var(\bm x_1)_{k \times k} & cov(\bm x_1, \bm x_2')_{k \times (n-k)} \\ 
cov(\bm x_2, \bm x_1')_{(n-k) \times k}& Var(\bm x_2)_{(n-k) \times (n-k)}
\end{bmatrix}
\end{array}
\end{equation*}

\item The correlation matrix is defined as

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}

corr(\bm x)_{n \times n} & = 
\begin{bmatrix}
1 & corr(X_1, X_2) & corr(X_1, X_3) & \hdots & corr(X_1, X_n) \\
corr(X_2, X_1) & 1 & corr(X_2, X_3) & \hdots & corr(X_2, X_n) \\
corr(X_3, X_1) & corr(X_3, X_2) & 1 & \hdots & corr(X_3, X_n) \\
\vdots & \vdots & \vdots & \ddots  & \vdots \\
corr(X_n, X_1) & corr(X_n, X_2) & corr(X_n, X_3) & \hdots & 1 \\
\end{bmatrix}
\end{array}
\end{equation*}

\item If $\bm X$ has a $n$-variate normal distribution, then, with
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{lll}

E(\bm x) = 
\bm \mu =
\begin{bmatrix}
\bm \mu_1 \\ \bm \mu_2 \\
\end{bmatrix}

 & \text{ and } & 
 
 Var(\bm x)_{n \times n} = 
\bm \Sigma =
\begin{bmatrix}
\bm \Sigma_1 & \bm \Sigma_{12} \\
\bm \Sigma_{21} & \bm \Sigma_2
\end{bmatrix}

\end{array}
\end{equation*}
then $\bm X_1 \mid \bm X_2$ has a $k$-variate normal distribution with
$$E(\bm X_1 \mid \bm X_2) = \bm \mu_1 + \bm \Sigma_{12} \bm
\Sigma_2^{-1}(\bm X_2 - \bm \mu_2)$$
$$Var(\bm X_1 \mid \bm X_2) = \bm \Sigma_1 - \bm \Sigma_{12} \bm \Sigma_2^{-1}
\bm \Sigma_{21}$$
\item[\bf{Note:}] $Var(\bm X_1 \mid \bm X_2) \leq Var(\bm X_1)$

\end{itemize}

\section{Computing using expected values and variances}
Let $a$, $b$ and $c$ be constants, and let $X$, $Y$ and $Z$ be (scalar) random
variables. The following rules hold regardless of the distribution of random
variables $X$, $Y$ and $Z$.

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}

E(c) = c \\
E(cX) = cE(X) \\
E(X+Y) = E(X)+E(Y) \\
E(X + c) = E(X)+c \\
E(XY) = E(X)E(Y) & \text{Only if $X \indep Y$.} \\
E(g(X)) = g(E(X)) & \text{Only in some special cases, like when $g(X)$ }\\
&\text{is a linear transformation.} \\
Var(X+Y) = Var(X)+Var(Y)+2cov(X,Y) \\
Var(X-Y) = Var(X)+Var(Y)-2cov(X,Y) \\
Var(aX) = a^2 \cdot Var(X) \\
cov(X, Y+Z) = cov(X,Y)+cov(X,Z) \\
cov(aX,bY) = ab\cdot cov(XY) \\
Var(X+a) = Var(X) \\
%TODO: Plot in notes 2 page 5 left out
cov(X+a,Y+b) = cov(X,Y) \\
E(X) = E_Y\left[E_{X\mid Y}(X \mid Y)\right] \\
Var(X) = E_Y\left[Var_{X\mid Y}(X \mid Y)\right]+Var_Y\left[Var_{X\mid Y}(X \mid
Y)\right]
\end{array}
\end{equation*}
Let $\bm a$ and $\bm b$ be fixed vectors and $\bm X$ and $\bm Y$ random vectors
so that the dimensions in the equations match.
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
E(\bm a' \bm X) = \bm a' E(\bm X) \\
Var(\bm a' \bm X) = \bm a' Var(\bm X) \bm a & \text{Compare to }Var(aX) = a^2
\cdot Var(X) = a \cdot Var(X) \cdot a \\
cov(\bm a' \bm X, \bm b' \bm Y) = \bm a' cov(\bm X, \bm Y) \bm b
\end{array}
\end{equation*}
{\bf Note: } These equations need to be remembered by hearth!

\chapter{Random samples}
\begin{definition}
\label{defRandomSamples1}
Random variables $X_1,..,X_n$ are called random sample of size $n$ from
population $f(x)$, if $X_1,\ldots,X_n$ are mutually independent random variables
and the marginal pdf/pmf if each $X_i$ is the same function $f(x)$.
Alternatively, $X_1,\ldots,X_n$ are called independent and identically
distributed random variables (i.i.d.) with pdf/pmf $f(x)$.
\end{definition}
\textbf{Note:} Sample $X_1,\ldots,X_n$ can also be denoted by $\bm X$, where
$\bm X = \begin{bmatrix}X_1 & \hdots & X_n \end{bmatrix}^T$
\begin{itemize}
\item If follows from the mutual independence, of $X_1,\ldots,X_n$ that the
joint pdf or pmf of $\bm X$ is
$$f(x_1,\ldots,x_2) = f(x_1)f(x_2)\ldots f(x_n) = \prod_{i=1}^n f(x_i)$$
\item All univariate marginal distributions $f(x_i)$ are the same by definition
\ref{defRandomSamples1}.
\end{itemize}
\begin{example}
Let $X_1,\ldots,X_n$ be a random sample from $Exponential(\beta)$ population.
$X_i$ specifies the time until failure for $n$ identical cellphones. The
exponential pdf is
$$f(x_i) = \frac{1}{\beta}e^{-x_i/\beta}$$ %TODO: Is the x_i correct?(note 2 p6)
The joint pdf pf the sample is
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
f(x_1, x_2, \ldots, x_n \mid \beta) = \prod_{i=1}^n
\frac{1}{\beta}e^{-x_i/\beta} = \frac{1}{\beta^n}e^{\frac{1}{\beta}\sum x_i} &
\text{Recall: } a^ba^c=a^{b+c}
\end{array}
\end{equation*}
What is the probability that all $n$ cellphones last more than 2 years?
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
P(X_1>2,\ldots,X_n>2) & = \int_2^\infty \ldots \int_2^\infty f(x_1,\ldots,x_n)
\ud x_1 \ldots \ud x_n \\
& = \int_2^\infty \ldots \int_2^\infty \prod_{i=1} \frac{1}{\beta}e^{-x_i/\beta}
\ud x_1 \ldots \ud x_n \\
& = \int_2^\infty \ldots \int_2^\infty \prod_{i=2} \frac{1}{\beta}e^{-x_i/\beta}
\underbrace{\int_2^\infty \frac{1}{\beta}e^{-x_1/\beta} \ud x_1}_{
\substack{
\text{Integral over the exponential pdf}\\
\int_2^\infty f(x_1) \ud
x_1 = 1 - F(2) \\ =
1-(1-e^{-\frac{1}{\beta}2}) = e^{\frac{-2}{\beta}}}} \ud x_2 \ldots \ud x_n
\\ %TODO: better way to do the underbrace?
& = e^{\frac{-2}{\beta}}\underbrace{\int_2^\infty \ldots \int_2^\infty
\prod_{i=2} \frac{1}{\beta}e^{-x_i/\beta} \ud x_2 \ldots \ud
x_n}_{\substack{\text{Identical to original integral except that} \\
\text{there are only n-1 terms to be integrated}}} =
\ldots
\\
& = e^{-2/\beta}\cdot e^{-2/\beta} \int_2^\infty \ldots \int_2^\infty
\prod_{i=3} \frac{1}{\beta}e^{-x_i/\beta} \ud x_3 \ldots \ud x_n \\
& = (e^{-2/\beta})^n = e^{-2n/\beta}
\end{array}
\end{equation*}
\end{example}
A much more simpler solution: notice that $P(X_i > 2) = 1 - F(2) =
e^{-1/\beta}$. Because $X_i$'s are independent, then also events $(X_i > 2)$ are
independent event, and so
$$P(\text{All } X_i > 2) = \prod_{i=1}^n P(X_i > 2) = (e^{-2/\beta})^n =
e^{-2n/\beta}$$
\textbf{Illustration} See \verb#ExponentialSample.R# for the case where $n=2$
and $\beta=3$. Notice that R uses parametrization $\lambda=\frac{1}{\beta}$
for the exponential distribution, so in R, exponential pdf is $f(x \mid
\lambda) = \lambda e^{-\lambda x}$.
%TODO: Left out the illustrations and texts related to illustrations in notes 3
% page 1 (end).

\textbf{Note:} Definition \ref{defRandomSamples1} assumes that $X_1,\ldots,X_n$
are independent. In practice, we often do analysis on dependent data. For that
use, we could define term ``dependent random sample''. Mathematical treatment of
dependent sample requires more specific description of dependence structure,
e.g. through spatial or temporal autocorrelation models, or explicit models for
grouped data.

Dependent data are modeled by assuming that random vectors $\bm
X_1,\ldots,\bm X_n$ are vectors of appropriate length, and there are $n$
independent realizations of them in the data. Quite often $n=1$ and each
replicate to $\bm X_1$, which is a rather long vector.
%TODO: Illustration about data points left out in notes 3, page 2. Is it needed?
\begin{example} % TODO: Remark about see exercise 7.4 of ISI1 left out. Is it
% needed? If so, see actual number of new notes.
\label{exampleSpatialData1}
Let $\bm X = \begin{bmatrix} X(u_1) & X(u_2) & X(u_3) \end{bmatrix}^T$ include
random variables $X$ at locations $u_1$, $u_2$ and $u_3$. Assume that $\bm X_1$
is normally distributed and the correlation $\rho_{ij} = corr(X(u_i), X(u_j))$
depends only on the spatial distance $s_{ij} = \lVert u_i - u_j \rVert$ between
locations $u_i$ and $u_j$.

The marginal means and variances of $X(u_i)$ are $E(X(u_i))=\mu$ for all $i$ and
$Var(X(u_i)) = \sigma^2$ for all $i$. Consider case where $u_1 = (1,0)$, $u_2 =
(0,0)$, $u_3=(0,2)$ and $\rho_{ij} = e^{-s_{ij}/2}$. Correlations and
covariances related to different random variables can be seen on table
\ref{exampleSpatialData1Table1}.

\begin{table}[h]
\caption{Correlations between the random variable pairs
of example \ref{exampleSpatialData1}}
\label{exampleSpatialData1Table1}
\renewcommand{\arraystretch}{1.4}
\renewcommand{\tabcolsep}{0.3cm}
\begin{tabular}{c|c|c|c}
Pair & $s_{ij}$ & $\rho_{ij}$ & $cov(X(u_i), X(u_j))$ \\
\hline
1,2 & 1 & 0.61 & $0.61 \sigma^2$ \\
1,3 & $\sqrt{5}$ & 0.33 & $0.33 \sigma^2$ \\
2,3 & 2 & 0.37 & $0.37 \sigma^2$ \\

\end{tabular}
\end{table}
%TODO: Illustration about data point location distances left out on notes 3 page
% 2. Is it needed?
Let
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{lll}
\bm \mu = \begin{bmatrix} \mu \\ \mu  \\ \mu  \end{bmatrix} &
\text{and} & 
\bm \Sigma = \sigma^2
\begin{bmatrix} 
1 & 0.61 & 0.33 \\ 
0.61 & 1 & 0.37  \\ 
0.33 & 0.37 & 1
\end{bmatrix}
\end{array}
\end{equation*}

The joint pdf of $\bm X$ is 3 variate normal distribution with mean $\bm \mu$
and variance $\bm \Simga $ 
$$f(x_1 \mid \bm \mu, \bm \Sigma) = \frac{1}{\sqrt{(2\pi)^3}}
\lvert \bm \Sigma \rvert^{-1/2}e^{\frac{1}{2}(\bm x_1 - \bm \mu)'\bm \Sigma^{-1}(\bm x_1 - \bm
\mu)}$$
\end{example}
\textbf{Note:} In random sample, we assume that $X_1,\ldots,X_n$ are identically
distributed. In the case of random vectors, whis is specified by saying that the
marginal distributions of the elements of $\bm X_i$ are identical.

\textbf{Note:} We can also have independent replicates of random vectors, e.g.
if we have grouped data where the groups are independent replicates from the
process that generates the groups, and the observations within the groups are
dependent. This is related to mixed-effects models.

\textbf{Note:} Definition \ref{defRandomSamples1} specifies sampling from an
infinite population (or population model): the sampling procedure does not
change the population.

The population may also be finite, like numbers in hat. In that case, the
sampling can be made with replacement: whenever a number has been drawn, the
value is recorded but the number is put back to the hat. This procedure, which
is useful e.g. in Bootstrapping, fulfils the conditions of definition
\ref{defRandomSamples1}.

If we do the sampling without replacement (which often makes much sense), then
the conditions of definition \ref{defRandomSamples1} are not fulfilled: each
draw changes the population by removing one unit from the finite population.
This leads to so called design-based inference, which is covered in the
literature of sampling theory.

The difference to definition \ref{defRandomSamples1} is important especially if
the sample size $n$ is large compared to the size of population, but may be
unimportant if $N >> n$. In this course, we are considering only sampling
according to definition \ref{defRandomSamples1}.

Random samples can be summarized to a well defined summary called statistic.

\begin{definition}
Let $\bm X = X_1,\ldots,X_n$ be a random sample of size $n$ from population
$f(x)$ and let $T(\bm X)$ be a real-valued or vector-valued function, whose
domain includes the sample space of $\bm X$. The random variable $Y = T(\bm X)$
is called a statistic (statistiikka, tunnusluku). The probability distribution
of $Y$ is called the sampling distribution of $Y$ (otantajakauma).
\end{definition}

\textbf{Examples of statistics}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
\text{Sample minimum} & min(\bm X) \\
\text{Sample maximum} & max(\bm X) \\
\text{Sample median} & median(\bm X) \\
\end{array}
\end{equation*}
Also other like sample mean, variance, standard deviation, quartiles etc.

\begin{example} Let $\bm X$ be a random sample of size 5 from $Exponential(2)$
population. R script \verb#statistics.R# illustrates the distributions of
$min(\bm X)$, $max(\bm X)$, $median(\bm X)$ and $mean(\bm X)$ through
simulation. Script repeates the sampling from Exponential distribution $M=10000$
times and illustrates the distribution of the above mentioned sample statistic
when $n=5$.
\end{example}

\begin{definition}
The sample mean is the statistic defined by
$$\bar X = \frac{1}{n}\sum_{i=1}^n X_i$$
\end{definition}

\begin{definition}
The sample variance is the statistic defined by
$$S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2$$
\end{definition}
\begin{itemize}
  \item The sample standard deviation is the statistic defined by $S =
  \sqrt{S^2}$.
  \item The observed values of these random variables are denoted by $\bar X$,
  $s^2$ and $s$.
\end{itemize}
\begin{theorem}
Let $X_1,\ldots,X_n$ be any numbers. Then
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{lll}
\text{a)} & min_a \sum_{i=1}^n (x_i-a)^2 = \sum_{i=1}^n(x_i-\bar x)^2 \\
\text{b)} & (n-1)s^2 = \sum_{i=1}^n (x_i-\bar x)^2 = \underbrace{\sum_{i=1}^n
x_i^2-n\bar x^2}_{\substack{\text{Compare to} \\ Var(\bm X) = E(\bm
X^2)-(E(\bm X))^2}} %TODO: Is this correct (in notes, \bm X and X mixed in
% formula)
\\
\end{array}
\end{equation*}
%TODO: Plot at notes 4 page 3 left out. Is it needed?
\end{theorem}
\end{document}
