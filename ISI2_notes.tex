%%%%%%%%  Document class  %%%%%%%%%%%%
\documentclass[10pt, twoside, a4paper]{book}

%%%%%%%%  Packages   %%%%%%%%%%%%%%%%
\usepackage{natbib}
%\usepackage{d:/laurim/biometria/lshort/src/lshort}
%\usepackage{numline}
%\usepackage{lineno}
\bibpunct{(}{)}{,}{a}{}{,}
\usepackage{amsmath,amssymb}
\usepackage{bm} 
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{color}
\usepackage{commath}

%\newenvironment{Rcode}[1]%
%{\scriptsize{\ttfamily{\bf The R-code for #1}} \\
%\ttfamily \noindent\ignorespaces }
%{\par\noindent%
%  \ignorespacesafterend}
  
\newenvironment{Rcode}[1]%
{\ttfamily{\bf The R-code for #1}
\scriptsize \\\par\verbatim}
{\endverbatim\par} 

\newenvironment{Rcode2}%
{\scriptsize \par\verbatim}%
{\endverbatim\par}

\usepackage{graphicx}
%\usepackage{subfigure}
%\usepackage[nolists]{endfloat}
\usepackage{amsthm}

\usepackage{times}
%\usepackage[T1]{fontenc}

%\usepackage{breqn} % Helps with multline \left \right problems.

\newcommand{\foldPath}{c:/laurim/tex/}

\newcommand{\beq}{\begin{equation*}}
\newcommand{\eeq}{\end{equation*}}
\newcommand{\ud}{\,\mathrm{d}}
\newcommand{\ureal}{\,\mathbb{R}}
\newcommand{\uninf}{\,-\infty}
\newcommand{\uinf}{\,-\infty}
\usepackage{graphicx}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\corr}{corr}
\DeclareMathOperator{\sd}{sd}

\theoremstyle{definition}

\newtheorem{example}{Example}[chapter]
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
%\bibpunct{(}{)}{,}{a}{}{,}
\bibliographystyle{\foldPath saf}
%\bibliographystyle{alpha}
%\bibliographystyle{\foldPath wiley}
%\bibliographystyle{unsrt}

%%%%%%%%  Page Setup %%%%%%%%%%%%%%%%%%
%\topmargin      0pt
%\headheight     0pt 
%\headsep        0pt 
%\textheight   648pt 
%\oddsidemargin  0pt
%\textwidth    468pt 

%%%%%%%% Document %%%%%%%%%%%%%%%%%%%
\begin{document}
\pagenumbering{Roman}
\setlength{\baselineskip}{16pt}

%% Front matter
%\title{Forest biometrics with examples in R}
\title{Introduction to statistical inference 2}

\author{Lauri Meht\"atalo\\
	   University of Eastern Finland \\ School of Computing
	   }

\date{\normalsize \today}
\maketitle
\tableofcontents

%\section*{Preface}

%These are lecture notes for course Biometrics: R-statistics in Forest Sciences. In writing these notes, I have used several sources. In writing chapter 1, the main sources have been the second edition of the classical book ``Statistical Inference'' by George Casella and Roger L Berger\citep{Casellaandberger2002} and ``Methods for Forest Biometrics'' by Juha Lappi  \citep{Lappi1993}. In addition, I have used lecture notes of courses on mathematical statistics (by Eero Korpelainen) and statistical inference (by Jukka Nyblom). For chapters 2 and 3, the main sources have been the lecture notes of Jukka Nyblom on Regression analysis and Linear models, the book of Jose Pinheiro and Douglas Bates on Linear models with R \citep{Pinheiroandbates2000}, and the book ``Generalized, Linear and Mixed Models'' by CharlesMcCulloch and Shayle R Searle \citep{McCullochandsearle2001}. For the GLMs and GLMMs, I have used \citet{McCullochandsearle2001}, too. The last chapter on models systems is based mainly on book ``Econometric analysis'' by William H Greene \citep{Greene1997}. In addition, good sources of information have been the lecture notes of Annika Kangas on Forest Biometrics, the books of of Julian Faraway \citep{Faraway2004, Faraway2006}, Keith E Muller and Paul W Stewart \citep{Mullerandstewart2006}, and William N. Venables and Brian D. Ripley \citep{Venablesandripley2002}.   

\mainmatter
\pagenumbering{arabic}

\chapter {Recap from ``Introduction to statistical inference 1''}
\section{Random variable}
\begin{itemize}
\item Random variable is a function from sample space $\mathcal{S}$ of an
experiment to sample space of the random variable $\mathcal{X}$, which is set of real numbers.
$$X: \mathcal{S} \rightarrow \mathcal{X}$$
\item The sample space is a set of all possible values random variable can get.
\item $\mathcal{X}$ can be
\begin{itemize}
  \item an interval of real axis (continuous random variable).
  $$\mathcal{X} = [0, 10),~ \mathcal{X} = [0, 10],~ \mathcal{X} = (0, 10)$$
  \item An uncountable set of integers (discrete random variable) 
  $$\mathcal{X} = \{0,1,2,\ldots\}$$
  \item A countable set of integers or real numbers (discrete random variable)
  $$\mathcal{X} =\{0,1\},~\mathcal{X} = \{0,0.5,1\},~ \mathcal{X} =
  \{0,1,\ldots,10\}$$ 
\end{itemize}
\item Probabilities associated with each value of $X$ are defined by the
cumulative distribution function (cdf for short).
$$F_X(x) = P(X \leq x),~\text{where } -\infty < x < \infty$$
\item[\bf{Note:}]$F_X(x)$ is a step function if $X$ is discrete.
\item[\bf{Note:}]$F_X(x)$ is a continuous function if $X$ is continuous.
%TODO: Two plots visualizing discreate and continous CDFs. Notes 1, page 2.
\item $F_X(x)$ or $F(x)$ is cdf, if
\begin{itemize}
  \item [1)] $\lim_{x \to -\infty} F(x) = 0$ and $\lim_{x \to
  \infty} F(x) = 1$
  \item [2)] $F(x)$ is non-decreasing
  \item [3)] $F(x)$ is right-continuous
\end{itemize}
\item Cdf is useful in calculation of any probabilities; for example
$$P(a < X \leq b) = F(b) - F(a)$$
\item [\bf{Note:}] Be careful with $<$ and $\leq$ when working with discreate
random variables.
\item The probability density function (pdf for short) is defined for
continuous random variable as
$$f_X(x) = F'(x) = \frac{\ud F_X(x)}{\ud x}, ~ -\infty < x < \infty$$
and
$$\int_{-\infty}^x f_X(t)\ud t = F_X(x)$$
\item The probability mass function (pmf for short) is defined for discrete
random variables as
$$f_X(x) = P(X=x)$$
$$F_X(x) = \sum_{k=1}^x f_X(k)$$
\end{itemize}
\section{Transformations of random variable}
\begin{itemize}
  \item Consider a monotonic function $g: \mathcal{X} \to \mathcal{Y}$
  \item $Y = g(X)$ is also a random variable; function $g$ is called an
  transformation (muunnos).
  \item If $g(x)$ is a increasing function of $x$, then
  $$F_Y(y) = F_X(g^{-1}(y))$$
  \item If $g(x)$ is a decreasing function of $x$, then
  $$F_Y(y) = 1 - F_X(g^{-1}(y))$$
  \item The pdf of continuous $Y$ is
  $$f_Y(y) = F_Y'(y)$$
\end{itemize}
\section{Expected values}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
E(X) = \mu_X = \left\{
\begin{array}{ll}
\int_{-\infty}^\infty x f(x) \ud x & \text{if $X$ is continuous} \\
\sum_{x \in \mathcal{X}} x f(x) & \text{if $X$ is discrete}
\end{array} \right.
\end{equation*}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
E(g(X)) =  \left\{
\begin{array}{ll}
\int_{-\infty}^\infty g(x) f(x) \ud x & \text{if $X$ is continuous} \\
\sum_{x \in \mathcal{X}} g(x) f(x) & \text{if $X$ is discrete}
\end{array} \right.
\end{equation*}
\section{Variance}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
\sigma_X^2 = Var(X) & = E(X - \mu_X)^2 \\
& = E(X^2 - 2X\mu_X + \mu_X^2) \\
& = E(X^2) - E(2X\mu_X) + E(\mu_X^2) \\
& = E(X^2) - \underbrace{2\mu_X \underbrace{E(X)}_{\mu_X}}_{2\mu_X^2} +
E(\mu_X^2) \\
& = E(X^2) - \mu_X^2 \\
\end{array}
\end{equation*}

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
sd(X) = \sqrt{Var(X)} = \sigma_X
\end{array}
\end{equation*}

\section{Bivariate random variables}
\begin{itemize}
  \item For two discrete random variables, the joint pmf is defined as
  $$f_{X,Y}(x,y)=P(X=x, Y=y)$$
  \item For two continuous random variables, we define the joint pdf
  $f_{X,Y}(x,y)$ as
  $$P((X,Y) \in A) = \iint\limits_A f(x,y) \ud x \ud y$$
  \item The expected value for transformation
$g(X,Y) : \ureal^2 \to \ureal$ (for example, \\ $g(X,Y) = XY$ or
$g(X,Y) =
\frac{X}{Y}$) is
$$E(g(X,Y)) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y)f(x,y) \ud
x \ud y$$
if $(X,Y)$ is continuous, and
$$E(g(X,Y)) = \sum_{x,y \in \ureal^2} g(x,y)f(x,y)$$
if $(X,Y)$ is discrete.
\item The marginal pmf / pdf for $X$ are
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
f_X(x) = \sum_{y \in \ureal} f_{X,Y}(x,y) & \text{(pmf)} \\
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \ud y & \text{(pdf)} \\
\end{array}
\end{equation*}
and correspondingly for $Y$.
\item The conditional pmf / pdf are both defined as
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
f(y \mid x)  = \frac{f_{X,Y}(x,y)}{f_X(x)} & \text{(for both discrete and
continuous random variables)}
\end{array}
\end{equation*}
and correspondingly for $x \mid y$.
\end{itemize}
\section{Independence}
\begin{itemize}
  \item Random variables are said to be independent ($X \indep Y$) if
  $$f_{X,Y}(x,y) = f_X(x)f_Y(y)$$
  \item For independent random variables, conditional distribution $y \mid x$ is 
  $$f(y \mid x) = f_y(y)$$
  regardless of the value of $x$.
\end{itemize}

\section{Covariance}
\begin{itemize}
  \item Covariance measures the linear association between two random variables
  $X$ and $Y$
  \begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{lll}
	cov(X,Y) & = E((X-\mu_X)(Y-\mu_Y)) \\
	& = E(XY) - \mu_X\mu_Y \\
	cov(X,Y) & = cov(Y,X) \\
	corr(X,Y) & = \rho_{XY} = \frac{cov(X,Y)}{\sigma_X \sigma_Y} & \text{where } -1
	\leq \rho_{XY} \leq 1
	\end{array}
	\end{equation*}
	\item[\bf{Note:}] $cov(X,X) = Var(X)$
	%TODO: Small plot left out. Notes 1, page 8.
	\item[\bf{Note:}] If $X \indep Y$, then $cov(X,Y) = 0$, but if $cov(X,Y) = 0$,
	it does not mean $X$ and $Y$ are necessarily independent.
\end{itemize}
\section{Random vectors}
\begin{itemize}
  \item Random vectors generalize the bivariate random variables to a
  $n$-variate case.
  	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	\bm X = \left [
	\begin{matrix}
	X_1 \\ X_2 \\ \vdots \\ X_n
	\end{matrix} \right ] & \text{where $X_i,~i=1,2,\ldots,n$ are scalar random
	variables}
	\end{array}
	\end{equation*}
	\item If $\bm x$ is a continuous random vector, then
	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	P(\bm X \in A) = \idotsint\limits_A f(\bm x) \ud x_1 \ldots \ud x_n &
	f: \ureal^2 \to \ureal
	\end{array}
	\end{equation*}
	where $f(\bm x)$ is a joint pdf.
	\item If $\bm x$ is a discrete random vector, then
	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	P(\bm X \in A) = \sum \ldots \sum f(\bm x)
	\end{array}
	\end{equation*}
	where $f(\bm x)$ is a joint pmf.

\item Let $g(\bm x)$ be a transformation $g: \ureal^2 \to \ureal$. Then, the
	expected value is
	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	E(g(\bm X)) = \left\{
	\begin{array}{ll}
	\int_{-\infty}^\infty \ldots \int_{-\infty}^\infty x f(x) \ud x_1 \ldots \ud
	x_n & \text{if $\bm X$ is continuous} \\
	\mathop{\sum\ldots\sum}_{\bm X \in \ureal^2}g(\bm x)f(\bm x)& \text{if $\bm X$
	is discrete} \end{array} \right.
	\end{equation*}
	\item Let us partition the $n$-variate random vector $\bm X$ as follows:
	$$\bm X = \left(\begin{matrix}\bm X_1 \\ \bm X_2\end{matrix}\right)$$
	where $\bm X_1$ has lenght $k$ and $\bm X_2$ has length $n-k$. 
	\item The joint pdf
	of $\bm X$ can be written as
	$$f(\bm x) = f(\bm x_1, \bm x_2)$$ 
	%TODO: In notes, X_1 and X_2 normal random variables. Is this correct?
	\item The marginal density of $\bm X_1$ is
	\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	f(\bm x_1) = \int\limits_{\ureal^{n-k}} f(\bm x_1, \bm x_2) \ud \bm x_2 &
	\text{if $\bm X$ is continuous} \\
	f(\bm x_1) = \sum\limits_{\ureal^{n-k}} f(\bm x_1, \bm x_2) &
	\text{if $\bm X$ is discrete}
	\end{array}
	\end{equation*}
	where $f(\bm x_1)$ is a $k$-variate pdf/pmf.
	
	%TODO: Left out notes 2 page 2 stuff about pdf and pmf sample spaces and plots,
	% since they are little out of context in notes. Are they needed?
	
	\item The conditional pdf/pmf for $\bm X_2 \mid \bm X_1$ is
	$$f(\bm X_2 \mid \bm X_1) = \frac{f(\bm X_1, \bm X_2)}{f(\bm X_2)}$$
	
	%TODO: Left out plots and table visualization from notes 2 page 2 (end), are
	% they needed?
	
	\item Expected value of random vectors is defined as vector
	
	\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}

E(\bm x) = 
\begin{bmatrix}
E(X_1) \\ E(X_2) \\ \vdots \\ E(X_n)
\end{bmatrix}_{n \times 1} = 
\begin{bmatrix}
E(\bm X_1) \\ E(\bm X_2)
\end{bmatrix} =
\begin{bmatrix}
\bm \mu_1 \\ \bm \mu_2 \\
\end{bmatrix}
\end{array}
\end{equation*}
	
	\item The variance of a random vector is $n \times n$ symmetric matrix called
variance-covariance matrix.

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}

Var(\bm x)_{n \times n} & = 
\begin{bmatrix}
Var(X_1) & cov(X_1, X_2) & cov(X_1, X_3) & \hdots & cov(X_1, X_n) \\
cov(X_2, X_1) & Var(X_2) & cov(X_2, X_3) & \hdots & cov(X_2, X_n) \\
cov(X_3, X_1) & cov(X_3, X_2) & Var(X_3) & \hdots & cov(X_3, X_n) \\
\vdots & \vdots & \vdots & \ddots  & \vdots \\
cov(X_n, X_1) & cov(X_n, X_2) & cov(X_n, X_3) & \hdots & Var(X_n) \\
\end{bmatrix} \\ \\
 & = 
 
\begin{bmatrix}
Var(\bm x_1)_{k \times k} & cov(\bm x_1, \bm x_2')_{k \times (n-k)} \\ 
cov(\bm x_2, \bm x_1')_{(n-k) \times k}& Var(\bm x_2)_{(n-k) \times (n-k)}
\end{bmatrix}
\end{array}
\end{equation*}

\item The correlation matrix is defined as

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}

corr(\bm x)_{n \times n} & = 
\begin{bmatrix}
1 & corr(X_1, X_2) & corr(X_1, X_3) & \hdots & corr(X_1, X_n) \\
corr(X_2, X_1) & 1 & corr(X_2, X_3) & \hdots & corr(X_2, X_n) \\
corr(X_3, X_1) & corr(X_3, X_2) & 1 & \hdots & corr(X_3, X_n) \\
\vdots & \vdots & \vdots & \ddots  & \vdots \\
corr(X_n, X_1) & corr(X_n, X_2) & corr(X_n, X_3) & \hdots & 1 \\
\end{bmatrix}
\end{array}
\end{equation*}

\item If $\bm X$ has a $n$-variate normal distribution, then, with
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{lll}

E(\bm x) = 
\bm \mu =
\begin{bmatrix}
\bm \mu_1 \\ \bm \mu_2 \\
\end{bmatrix}

 & \text{ and } & 
 
 Var(\bm x)_{n \times n} = 
\bm \Sigma =
\begin{bmatrix}
\bm \Sigma_1 & \bm \Sigma_{12} \\
\bm \Sigma_{21} & \bm \Sigma_2
\end{bmatrix}

\end{array}
\end{equation*}
then $\bm X_1 \mid \bm X_2$ has a $k$-variate normal distribution with
$$E(\bm X_1 \mid \bm X_2) = \bm \mu_1 + \bm \Sigma_{12} \bm
\Sigma_2^{-1}(\bm X_2 - \bm \mu_2)$$
$$Var(\bm X_1 \mid \bm X_2) = \bm \Sigma_1 - \bm \Sigma_{12} \bm \Sigma_2^{-1}
\bm \Sigma_{21}$$
\item[\bf{Note:}] $Var(\bm X_1 \mid \bm X_2) \leq Var(\bm X_1)$

\end{itemize}

\section{Computing using expected values and variances}
Let $a$, $b$ and $c$ be constants, and let $X$, $Y$ and $Z$ be (scalar) random
variables. The following rules hold regardless of the distribution of random
variables $X$, $Y$ and $Z$.

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}

E(c) = c \\
E(cX) = cE(X) \\
E(X+Y) = E(X)+E(Y) \\
E(X + c) = E(X)+c \\
E(XY) = E(X)E(Y) & \text{Only if $X \indep Y$.} \\
E(g(X)) = g(E(X)) & \text{Only in some special cases, like when $g(X)$ }\\
&\text{is a linear transformation.} \\
Var(X+Y) = Var(X)+Var(Y)+2cov(X,Y) \\
Var(X-Y) = Var(X)+Var(Y)-2cov(X,Y) \\
Var(aX) = a^2 \cdot Var(X) \\
cov(X, Y+Z) = cov(X,Y)+cov(X,Z) \\
cov(aX,bY) = ab\cdot cov(XY) \\
Var(X+a) = Var(X) \\
%TODO: Plot in notes 2 page 5 left out
cov(X+a,Y+b) = cov(X,Y) \\
E(X) = E_Y\left[E_{X\mid Y}(X \mid Y)\right] \\
Var(X) = E_Y\left[Var_{X\mid Y}(X \mid Y)\right]+Var_Y\left[Var_{X\mid Y}(X \mid
Y)\right]
\end{array}
\end{equation*}
Let $\bm a$ and $\bm b$ be fixed vectors and $\bm X$ and $\bm Y$ random vectors
so that the dimensions in the equations match.
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
E(\bm a' \bm X) = \bm a' E(\bm X) \\
Var(\bm a' \bm X) = \bm a' Var(\bm X) \bm a & \text{Compare to }Var(aX) = a^2
\cdot Var(X) = a \cdot Var(X) \cdot a \\
cov(\bm a' \bm X, \bm b' \bm Y) = \bm a' cov(\bm X, \bm Y) \bm b
\end{array}
\end{equation*}
{\bf Note: } These equations need to be remembered by hearth!

\chapter{Random samples}
\begin{definition}
\label{defRandomSamples1}
Random variables $X_1,..,X_n$ are called random sample of size $n$ from
population $f(x)$, if $X_1,\ldots,X_n$ are mutually independent random variables
and the marginal pdf/pmf if each $X_i$ is the same function $f(x)$.
Alternatively, $X_1,\ldots,X_n$ are called independent and identically
distributed random variables (i.i.d.) with pdf/pmf $f(x)$.
\end{definition}
\textbf{Note:} Sample $X_1,\ldots,X_n$ can also be denoted by $\bm X$, where
$\bm X = \begin{bmatrix}X_1 & \hdots & X_n \end{bmatrix}^T$
\begin{itemize}
\item If follows from the mutual independence, of $X_1,\ldots,X_n$ that the
joint pdf or pmf of $\bm X$ is
$$f(x_1,\ldots,x_2) = f(x_1)f(x_2)\ldots f(x_n) = \prod_{i=1}^n f(x_i)$$
\item All univariate marginal distributions $f(x_i)$ are the same by definition
\ref{defRandomSamples1}.
\end{itemize}
\begin{example}
Let $X_1,\ldots,X_n$ be a random sample from $Exponential(\beta)$ population.
$X_i$ specifies the time until failure for $n$ identical cellphones. The
exponential pdf is
$$f(x_i) = \frac{1}{\beta}e^{-x_i/\beta}$$ %TODO: Is the x_i correct?(note 2 p6)
The joint pdf pf the sample is
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
f(x_1, x_2, \ldots, x_n \mid \beta) = \prod_{i=1}^n
\frac{1}{\beta}e^{-x_i/\beta} = \frac{1}{\beta^n}e^{\frac{1}{\beta}\sum x_i} &
\text{Recall: } a^ba^c=a^{b+c}
\end{array}
\end{equation*}
What is the probability that all $n$ cellphones last more than 2 years?
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
P(X_1>2,\ldots,X_n>2) & = \int_2^\infty \ldots \int_2^\infty f(x_1,\ldots,x_n)
\ud x_1 \ldots \ud x_n \\
& = \int_2^\infty \ldots \int_2^\infty \prod_{i=1} \frac{1}{\beta}e^{-x_i/\beta}
\ud x_1 \ldots \ud x_n \\
& = \int_2^\infty \ldots \int_2^\infty \prod_{i=2} \frac{1}{\beta}e^{-x_i/\beta}
\underbrace{\int_2^\infty \frac{1}{\beta}e^{-x_1/\beta} \ud x_1}_{
\substack{
\text{Integral over the exponential pdf}\\
\int_2^\infty f(x_1) \ud
x_1 = 1 - F(2) \\ =
1-(1-e^{-\frac{1}{\beta}2}) = e^{\frac{-2}{\beta}}}} \ud x_2 \ldots \ud x_n
\\ %TODO: better way to do the underbrace?
& = e^{\frac{-2}{\beta}}\underbrace{\int_2^\infty \ldots \int_2^\infty
\prod_{i=2} \frac{1}{\beta}e^{-x_i/\beta} \ud x_2 \ldots \ud
x_n}_{\substack{\text{Identical to original integral except that} \\
\text{there are only n-1 terms to be integrated}}} =
\ldots
\\
& = e^{-2/\beta}\cdot e^{-2/\beta} \int_2^\infty \ldots \int_2^\infty
\prod_{i=3} \frac{1}{\beta}e^{-x_i/\beta} \ud x_3 \ldots \ud x_n \\
& = (e^{-2/\beta})^n = e^{-2n/\beta}
\end{array}
\end{equation*}
\end{example}
A much more simpler solution: notice that $P(X_i > 2) = 1 - F(2) =
e^{-1/\beta}$. Because $X_i$'s are independent, then also events $(X_i > 2)$ are
independent event, and so
$$P(\text{All } X_i > 2) = \prod_{i=1}^n P(X_i > 2) = (e^{-2/\beta})^n =
e^{-2n/\beta}$$
\textbf{Illustration} See \verb#ExponentialSample.R# for the case where $n=2$
and $\beta=3$. Notice that R uses parametrization $\lambda=\frac{1}{\beta}$
for the exponential distribution, so in R, exponential pdf is $f(x \mid
\lambda) = \lambda e^{-\lambda x}$.
%TODO: Left out the illustrations and texts related to illustrations in notes 3
% page 1 (end).

\textbf{Note:} Definition \ref{defRandomSamples1} assumes that $X_1,\ldots,X_n$
are independent. In practice, we often do analysis on dependent data. For that
use, we could define term ``dependent random sample''. Mathematical treatment of
dependent sample requires more specific description of dependence structure,
e.g. through spatial or temporal autocorrelation models, or explicit models for
grouped data.

Dependent data are modeled by assuming that random vectors $\bm
X_1,\ldots,\bm X_n$ are vectors of appropriate length, and there are $n$
independent realizations of them in the data. Quite often $n=1$ and each
replicate to $\bm X_1$, which is a rather long vector.
%TODO: Illustration about data points left out in notes 3, page 2. Is it needed?
\begin{example} % TODO: Remark about see exercise 7.4 of ISI1 left out. Is it
% needed? If so, see actual number of new notes.
\label{exampleSpatialData1}
Let $\bm X = \begin{bmatrix} X(u_1) & X(u_2) & X(u_3) \end{bmatrix}^T$ include
random variables $X$ at locations $u_1$, $u_2$ and $u_3$. Assume that $\bm X_1$
is normally distributed and the correlation $\rho_{ij} = corr(X(u_i), X(u_j))$
depends only on the spatial distance $s_{ij} = \lVert u_i - u_j \rVert$ between
locations $u_i$ and $u_j$.

The marginal means and variances of $X(u_i)$ are $E(X(u_i))=\mu$ for all $i$ and
$Var(X(u_i)) = \sigma^2$ for all $i$. Consider case where $u_1 = (1,0)$, $u_2 =
(0,0)$, $u_3=(0,2)$ and $\rho_{ij} = e^{-s_{ij}/2}$. Correlations and
covariances related to different random variables can be seen on table
\ref{exampleSpatialData1Table1}.

\begin{table}[h]
\caption{Correlations between the random variable pairs
of example \ref{exampleSpatialData1}}
\label{exampleSpatialData1Table1}
\renewcommand{\arraystretch}{1.4}
\renewcommand{\tabcolsep}{0.3cm}
\begin{tabular}{c|c|c|c}
Pair & $s_{ij}$ & $\rho_{ij}$ & $cov(X(u_i), X(u_j))$ \\
\hline
1,2 & 1 & 0.61 & $0.61 \sigma^2$ \\
1,3 & $\sqrt{5}$ & 0.33 & $0.33 \sigma^2$ \\
2,3 & 2 & 0.37 & $0.37 \sigma^2$ \\

\end{tabular}
\end{table}
%TODO: Illustration about data point location distances left out on notes 3 page
% 2. Is it needed?
Let
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{lll}
\bm \mu = \begin{bmatrix} \mu \\ \mu  \\ \mu  \end{bmatrix} &
\text{and} & 
\bm \Sigma = \sigma^2
\begin{bmatrix} 
1 & 0.61 & 0.33 \\ 
0.61 & 1 & 0.37  \\ 
0.33 & 0.37 & 1
\end{bmatrix}
\end{array}
\end{equation*}

The joint pdf of $\bm X$ is 3 variate normal distribution with mean $\bm \mu$
and variance $\bm \Sigma$ 
$$f(x_1 \mid \bm \mu, \bm \Sigma) = \frac{1}{\sqrt{(2\pi)^3}}
\lvert \bm \Sigma \rvert^{-1/2}e^{\frac{1}{2}(\bm x_1 - \bm \mu)'\bm \Sigma^{-1}(\bm x_1 - \bm
\mu)}$$
\end{example}
\textbf{Note:} In random sample, we assume that $X_1,\ldots,X_n$ are identically
distributed. In the case of random vectors, whis is specified by saying that the
marginal distributions of the elements of $\bm X_i$ are identical.

\textbf{Note:} We can also have independent replicates of random vectors, e.g.
if we have grouped data where the groups are independent replicates from the
process that generates the groups, and the observations within the groups are
dependent. This is related to mixed-effects models.

\textbf{Note:} Definition \ref{defRandomSamples1} specifies sampling from an
infinite population (or population model): the sampling procedure does not
change the population.

The population may also be finite, like numbers in hat. In that case, the
sampling can be made with replacement: whenever a number has been drawn, the
value is recorded but the number is put back to the hat. This procedure, which
is useful e.g. in Bootstrapping, fulfils the conditions of definition
\ref{defRandomSamples1}.

If we do the sampling without replacement (which often makes much sense), then
the conditions of definition \ref{defRandomSamples1} are not fulfilled: each
draw changes the population by removing one unit from the finite population.
This leads to so called design-based inference, which is covered in the
literature of sampling theory.

The difference to definition \ref{defRandomSamples1} is important especially if
the sample size $n$ is large compared to the size of population, but may be
unimportant if $N >> n$. In this course, we are considering only sampling
according to definition \ref{defRandomSamples1}.

Random samples can be summarized to a well defined summary called statistic.

\begin{definition}
Let $\bm X = X_1,\ldots,X_n$ be a random sample of size $n$ from population
$f(x)$ and let $T(\bm X)$ be a real-valued or vector-valued function, whose
domain includes the sample space of $\bm X$. The random variable $Y = T(\bm X)$
is called a statistic (statistiikka, tunnusluku). The probability distribution
of $Y$ is called the sampling distribution of $Y$ (otantajakauma).
\end{definition}

\textbf{Examples of statistics}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
\text{Sample minimum} & min(\bm X) \\
\text{Sample maximum} & max(\bm X) \\
\text{Sample median} & median(\bm X) \\
\end{array}
\end{equation*}
Also other like sample mean, variance, standard deviation, quartiles etc.

\begin{example} Let $\bm X$ be a random sample of size 5 from $Exponential(2)$
population. R script \verb#statistics.R# illustrates the distributions of
$min(\bm X)$, $max(\bm X)$, $median(\bm X)$ and $mean(\bm X)$ through
simulation. Script repeates the sampling from Exponential distribution $M=10000$
times and illustrates the distribution of the above mentioned sample statistic
when $n=5$.
\end{example}

\begin{definition}
The sample mean is the statistic defined by
$$\bar X = \frac{1}{n}\sum_{i=1}^n X_i$$
\end{definition}

\begin{definition}
The sample variance is the statistic defined by
$$S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2$$
\end{definition}
\begin{itemize}
  \item The sample standard deviation is the statistic defined by $S =
  \sqrt{S^2}$.
  \item The observed values of these random variables are denoted by $\bar X$,
  $s^2$ and $s$.
\end{itemize}
\begin{theorem}
\label{theoremXbarSigma}
Let $X_1,\ldots,X_n$ be any numbers. Then
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{lll}
\text{a)} & min_a \sum_{i=1}^n (x_i-a)^2 = \sum_{i=1}^n(x_i-\bar x)^2 \\
\text{b)} & (n-1)s^2 = \sum_{i=1}^n (x_i-\bar x)^2 = \underbrace{\sum_{i=1}^n
x_i^2-n\bar x^2}_{\substack{\text{Compare to} \\ Var(\bm X) = E(\bm
X^2)-(E(\bm X))^2}} %TODO: Is this correct (in notes, \bm X and X mixed in
% formula)
\\
\end{array}
\end{equation*}
%TODO: Plot at notes 4 page 3 left out. Is it needed?
\end{theorem}

\begin{lemma} Let $X_1,\ldots,X_n$ be a random sample form a population and the
$g(X)$ be a function such that $E(g(X_i))$ and $Var(g(X_i))$ exist. Then
$$E\left[\sum_{i=1}^ng(X_i)\right] = n\cdot E(g(X_i))$$
and
$$Var(\sum_{i=1}^n g(X_i)) = n \cdot Var(g(X_i))$$
\textbf{Proof} 

$$E(\sum g(X_i)) = \sum E(g(X_i)) = *$$
Because $X_i$'s are identically distributed
$$*=nE(g(X_i))$$

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
Var(\sum g(X_i)) & = E\left[ \sum g(X_i) - E(\sum g(X_i)) \right]^2 \\
& = E\left\{ \sum \left[\underbrace{g(X_i) - E(g(X_i))}_{t_i}\right] \right\}^2
= * \\
\end{array}
\end{equation*}
With $t_i$, we can get following
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{l}
E(\sum t_i \sum t_i) \\
= E(\sum t_i^2 + \substack{\sum\sum\\i \neq j} t_i t_j) \\
= \sum E(t_i^2) + \substack{\sum\sum\\i \neq j}E(t_i t_j)
\end{array}
\end{equation*}
which we can expand back to original equation
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{l}
* =  \underbrace{\sum E(\left[ g(X_i) - E(g(X_i)) \right]^2)}_{\sum E(t_i^2)} +
\underbrace{\substack{\sum\sum\\i \neq j}E\left[(g(X_i) - E(g(X_i)))(g(X_j) -
E(g(X_j)))\right]}_{\substack{\sum\sum\\i \neq j}E(t_i t_j)} \\
~= \sum Var(g(X_i)) + \substack{\sum\sum\\i \neq j} cov(g(X_i),g(X_j)) \\
~= \underbrace{n \cdot Var(X_1)}_{
\substack{\text{Because $X_i$'s}
\\ \text{ identically distributed}}} 
+ n(n-1)\cdot \underbrace{0}_{\substack{\text{Because $X_i$'s} \\ \text{are
independent}}} = n \cdot Var\left[g(X_1)\right]
\end{array}
\end{equation*}
\end{lemma}
\textbf{Note:} The proof of $E(\sum g(X_i))$ did not utilize the assumption of
independence. Therefore it is valid also for dependent samples. The poof of
$Var(\sum g(X_i))$ used the assumption of independence. Therefore it is not
valid for dependent samples.
\begin{itemize}
  \item For dependent samples, we get
  $$Var(\sum g(X_i))= \sum_{i=1}^n \sum_{j=1}^n cov\left[g(X_i),g(X_j)\right]$$
  \item Proof left as an exercise (recall, that $Var(X_i) = cov(X_i, X_i)$).
  %TODO:  Is it still an exercise?

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}

Var(\bm x) & = 
\begin{bmatrix}
Var(X_1) & cov(X_1, X_2)  & \hdots & cov(X_1, X_n) \\
cov(X_2, X_1) & Var(X_2)  & \hdots & cov(X_2, X_n) \\
\vdots & \vdots & \ddots  & \vdots \\
cov(X_n, X_1) & cov(X_n, X_2) & \hdots & Var(X_n) \\
\end{bmatrix} \\
Var(\sum X_i) = \sum\sum cov(X_i, X_j)
\end{array}
\end{equation*}

\item That is, the sum of all elements of matrix $Var(\bm X)$.
\item[\textbf{Note:}] This extends the rule
$$Var(X_1 + X_2) = Var(X_1)+Var(X_2)+2\cdot cov(X_1, X_2)$$
to general sum $\sum X_i$.
\end{itemize}
\begin{theorem}
\label{theoremUnbiasedEstimators1}
Let $X_1,\ldots,X_n$ be a random sample form a population with mean $\mu$ and
variance $\sigma^2 < \infty$. Then
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{lll}
\text{a)} & E(\bar X) = \mu \\
\text{b)} & Var(\bar X) = \frac{\sigma^2}{n} & ~E(\bar X - \mu)^2 \\ 
\text{c)} & E(S^2) = \sigma^2 \\ 
\end{array}
\end{equation*}
\end{theorem}
\begin{itemize}
  \item Proof for a and b are familiar from ISI1 course. Proof of c applier the theorem
\ref{theoremXbarSigma} and is left as an exercise. %TODO: Still an exercise?
\item[\bf{Note:}] Because $E(\bar X)= \mu$, the statistic $\bar X$ is said to be
an unbiased estimator (harhaton estimaattori) of population mean $\mu$. Also 
$S^2$ is an unbiased estimator of population variance $\sigma^2$.
\end{itemize}
\begin{example}
Illustrate these results through simulation. Intuitively, mean of an observed
value of a statistic over a large number of samples should be close to expected
value of the statistic in question. Therefore, (a), (b) and (c) of theorem
\ref{theoremUnbiasedEstimators1} can be demonstrated by simulating $M$ samples
of a fixed size $n$ and exploring how the means of the sample values of $\bar
x$, $(\bar x - \mu)^2$ and $s^2$ behave as $M \to \infty$.

R-script \verb#demonstratebias.R# implements this by assuming that the
population has the $Uniform(0,10)$ distribution and $n=10$.
\end{example}
\section{Distribution of sum}
\begin{theorem}\label{theoremConvolutionFormula1} (Convolution formula)

If $X$ and $Y$ are independent, continuous random variables with pdf's $f_X(x)$
and $f_Y(y)$, then the pdf of the sum $Z = x + Y$ is
$$f_Z(z) = \int_{-\infty}^\infty f_X(w)f_Y(z-w) \ud w$$
\end{theorem}
\textbf{Proof: } See Casella \& Berger, p 215-216.

\textbf{Illustration: } See example 1.20 of \verb#notes.pdf#
  %TODO: Two plots in the end of notes 4 page 7 left out. Needed?

\textbf{Note:} To compute the complete distribution of a sum $Z = \sum_{i=1}^n$,
where $X_i$'s do not need to be identically distributed, but they are
independent, we need to apply theorem \ref{theoremConvolutionFormula1}
iteratively. E.g. to find the distribution of $X_1 + X_2 + X_3$, you may first
find the distribution of $Z = X_1 + X_2$ using theorem
\ref{theoremConvolutionFormula1} and thereafter use theorem
\ref{theoremConvolutionFormula1} again to find the pdf of $Z + X_3$. This is not
trivial in very general case, but there are easier ways to do this, e.g. when
$X_i$'s are identically distributed and independent (i.i.d).

However, recall that in any case, the moments are easy
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
E(\sum X_i) = \sum E(X_i) \\
Var(\sum X_i) = \sum Var(X_i) & \text{(If $X_i$'s are independent)}
\end{array}
\end{equation*}
\section{Sampling from Normally distributed population}
\begin{theorem}
\label{theoremSumOfRVs1}
Let $\bm X_{n \times 1}$ be random sample of size $n$ from a $N(\mu, \sigma^2)$
distribution and let $\bar X = \frac{1}{n}\sum_{i=1}^n X_i$ and $S^2 =
\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar X)^2$. Then
%TODO: Plot at notes 5 page 2 left out. Needed?
\begin{itemize}
  \item[a)] $\bar X$ and $S^2$ are independent random variables.
  \item[b)] $\bar X$ has $N(\mu, \frac{\sigma^2}{n})$
  \item[c)] $(n-1)S^2/\sigma^2$ has chi-squared distribution with parameter
  $n-1$. Parameter $n-1$ is commonly called the ``degrees of freedom''.
\end{itemize}
\end{theorem}
\textbf{Note:} The chi-squared distribution with $p$ degrees of freedom has the
pdf
$$f_X(x)= \frac{1}{\underbrace{\Gamma(p/2)}_{\text{Gamma
-function}}}X^{p/2-1}e^{-x/2}$$
\begin{itemize}
  \item Gamma -function: $\Gamma(z) = \int_0^\infty x^{z-1}e^{-x}\ud x$
\end{itemize}
\begin{lemma}
We use notation $\chi_p^2$ for a chi-squared random variables with $p$ degrees
of freedom
\begin{itemize}
  \item If $Z$ is a $N(0,1)$ random variable, then $Z^2 \sim \chi_1^2$, that is,
  the square of a standard normal random variable is a chi-squared random
  variable. %TODO: Is Z^2 \chi_p^2 or \chi_1^2?
  \item If $X_1,\ldots,X_n$ are independent and $X_i \sim \chi_{p_i}^2$ then
  $\sum X_i \sim \chi_{\sum p_i}^2$. That is, independent chi-squared random
  variables add to a chi-squared random variable, and the degrees of freedom
  also add.
\end{itemize}
\end{lemma}
\begin{example}
Illustrate the result of theorem \ref{theoremSumOfRVs1} using R-script. In file
\verb#normalchi.R#, we implement the following:

Repeat M times
\begin{itemize}
  \item[1.] Generate random variables $X_i \sim N(0,1)$, where $n=1,\ldots,9$
  \item[2.] Compute $Z = \sum x_i^2$ for each sample
  \item[3.] Plot the histogram of the $M$ obtained values of $Z$ and compare to
  the distribution $\chi_n^2$.
\end{itemize}
\end{example}
\textbf{Note:} If $\bm X_n$ is a random sample from $N(\mu,\sigma^2)$
population, then the random variable
$$Z = \frac{\bar X - \mu}{\sigma / \sqrt{n}} \sim N(0,1)$$
where $\mu = E(\bar X)$ and $\sigma / \sqrt{n} = Var(\bar X)$.
\begin{itemize}
  \item This transformation could be used to make inference on $\mu$ using the
  observed $\bar x$, if $\sigma^2$ is known.
  \item However, that is not the case, and therefore we use $\frac{\bar X -
  \mu}{\sigma / \sqrt{n}}$ which can also be written as
  $$\frac{(\bar X - \mu)/(\sigma / \sqrt{n})}{\sqrt{S^2 / \sigma^2}}$$
  where $(\bar X - \mu)/(\sigma / \sqrt{n}) \sim N(0,1)$ and
  $\sqrt{S^2 / \sigma^2} \sim \chi_{n-1}^2$.
\end{itemize}
\begin{theorem}
Let $\bm X$ be a random sample from $N(\mu, \sigma^2)$. The random variable
$$T = \frac{\bar X - \mu}{S/\sqrt{n}}$$
has Student's t-distribution with parameter (n-1) ``degrees of freedom''. 

In
general, a random variable $T$ has Student's t-distribution with $p$ degrees of
freedom ($T \sim t_p$) if $T$ has the pdf
$$f_T(t) =
\frac{\Gamma(\frac{p+1}{2})}{\Gamma(\frac{p}{2})}\frac{1}{(p\pi)^{1/2}}\frac{1}{(1+t^2/p)^{(p+1)/2}}$$
\end{theorem}
\textbf{Proof:} See Casella \& Berger, p.223-224.

\textbf{Illustration:} File \verb#normalchi.R#
\begin{itemize}
  \item We use samples of sizes $3,4,\ldots,100$ and $M=10000$.
  \item Plot empirical histogram of $\frac{\bar X - \mu}{s/\sqrt{n}}$
  \item Compare to $t_{n-1}$ distribution and to $N(0,\sigma^2)$.
  \item $\mu=5$ and $\sigma^2 = 3^2$. %TODO: Is this enough information?
  \item Because $S^2 \to \sigma^2$ as $n$ increases, the difference between
  $N(0,1)$ and $t_{n-1}$ becomes meaningless as $n \to \infty$.
\end{itemize}
\textbf{Note:} $t_p$ has only $p$ moments. Especially $E(T_p) = 0$ if $p > 1$.
%TODO: Was there someting that was left out of page in notes 5 page 3 end?

\begin{itemize}
  \item A third important derived distribution under sampling from a
normal population is the Snedecor's/Fisher's F-distribution. 
\item It is the theoretical distribution of
the ratio of variances.
\end{itemize}
\begin{example}
Let $\bm X$ be a random sample from $N(\mu_X,\sigma_X^2)$ and let $\bm Y$ be a
random sample from $N(\mu_Y,\sigma_Y^2)$. If we were interested in comparing the
variability in these two populations, a quality of interest would be
$\sigma_X^2/\sigma_Y^2$. Information about $\sigma_X^2/\sigma_Y^2$ is contained
in $s_X^2/s_Y^2$; ratio of the sample variances.

The F-distribution allows this comparison by giving te distribution for random
variable
$$\frac{s_X^2/s_Y^2}{\sigma_X^2/\sigma_Y^2} =
\frac{s_X^2/\sigma_X^2}{s_Y^2/\sigma_Y^2} $$
where ratios $s_X^2/\sigma_X^2$ and $s_Y^2/\sigma_Y^2$ are independent, scaled
$\chi^2$-distributed random variables. 
\end{example}

\begin{definition}
Let $\bm X_{n \times 1}$ be a random sample from a $N(\mu_X, \sigma_X^2)$
population and let $\bm Y_{m \times 1}$ be a random sample from an independent
$N(\mu_Y, \sigma_X^2)$ population. The random variable
$$F = \frac{s_X^2/\sigma_X^2}{s_Y^2/\sigma_Y^2} $$
has the F-distribution with parameters (numerator and denominator degrees of
freedom) $n-1$ and $m-1$. Equivalently, random variable $F$ has the
F-distribution with $p$ and $q$ degrees of freedom $(F \sim F_{p,q})$ if $F$ has
the pdf
$$f_F(x) =
\frac{\Gamma(\frac{p+q}{2})}{\Gamma(\frac{p}{2})\Gamma(\frac{q}{2})}\left(\frac{p}{q}\right)^{p/2}\frac{x^{p/2-1}}{\left[1+\frac{p}{q}x\right]^{(p+q)/2}}$$
\end{definition}
\textbf{Note:} \begin{itemize}
  \item[a)] If $X \sim F_{p,q}$, then $1/X \sim F_{q,p}$
  \item[b)] If $X \sim t_q$, then $X^2 \sim \chi_1^2$ (recall that if $X \sim
  N(0,1)$, then $X^2 \sim \chi_1^2$)
\end{itemize}
\section{Convergence}
\begin{itemize}
  \item What happens if $n \to \infty$.
\end{itemize}
\begin{theorem}
Markov inequality

Let $X$ be a random variable such that $P(X \geq 0) = 1$ (i.e. $X$ gets only
positive values). Then, for every number $t > 0$
$$P(x > t) \leq \frac{E(X)}{t}$$
% TODO: Left out plot in notes 6 page 1. Is it needed?
\textbf{Proof:} Consider only case where $X$ is discrete random variable.
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
E(X) & = \sum_X x f(x) \\
& = \sum_{x<t}x f(x) + \sum_{x \geq t} x f(x) \\
\end{array}
\end{equation*}
Because $X \geq 0$, both terms are positive
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
E(X) & \geq \sum_{X \geq t}xf(x) \geq \sum_{X \geq t} t f(x) = t
\underbrace{\sum_{X \geq t} f(x)}_{P(X\geq t)} \\
& = t \cdot P(X\geq t) \\
& \\
E(X) & \geq t \cdot P(X\geq t) || :t ~~ (>0)\\
\frac{E(X)}{t} & \geq P(X\geq t)
\end{array}
\end{equation*}
\end{theorem}
\begin{example} Let $X$ be a non-negative random variable with $E(X) = 1$
$$P(X \geq 100) \leq \frac{E(X)}{100} = 0.01$$
\end{example}
\begin{theorem} Chebyshev's inequality

Let $X$ be a random variable such that $Var(X)$ exists. Then for every number
$t > 0$
$$P(\abs{X-E(X)} \geq t) \leq \frac{\sigma^2}{t^2}$$
\textbf{Proof:} Let $Y=(X-E(X))^2$. Now $P(Y \geq 0) = 1$ and $E(Y) = Var(X)$
$$P(\abs{X - E(X)} \geq t) = P(Y \geq t^2) \leq \frac{E(Y)}{t^2}$$
$$P(\abs{X - E(X)} \geq t) \leq \frac{Var(X)}{t^2}$$
\end{theorem}
\begin{example} If $Var(X) = \sigma^2$ and we select $t=3\sigma$
$$P(\abs{X-E(X)} \geq 3 \sigma) \leq \frac{\sigma^2}{(3\sigma)^2} =
\frac{1}{9}$$
\end{example}
\begin{definition}
A sequence of random variables $X_1,X_2,\ldots$ converges in probability
(konvergoi todennäköisyysmielessä) to a random variable $X$ if for every
$\epsilon > 0$
$$\lim_{n \to \infty} P\left(|X_n-X| \geq \epsilon \right) = 0$$
or equivalently
$$\lim_{n \to \infty} P\left(|X_n-X| < \epsilon \right) = 1$$
A commonly used notation for this kind of convergence is $X_n
\xrightarrow{P} X$. The sequence $X_1,\ldots,X_n$ is not usually an i.i.d.
sample but e.g. a statistic based on a sample of size $n$. Also, $X$ is often a
fixed constant, as it is in the following theorem.
\end{definition}
\begin{theorem}
(The weak law of large numbers (Heikko suurten lukujen laki), WLLN)

Let $X_1,X_2,\ldots$ be i.i.d. random variables with expected value $E(X_i) =
\mu$ and variance $Var(X_i)=\sigma^2 < \infty$. Define $\bar{X}_n =
\frac{1}{n}\sum_{i=1}^n X_i$. Then, for every $\epsilon > 0$
$$ \lim_{n \to \infty} P(|\bar{X_n}-\mu|< \epsilon) = 1$$
meaning sample mean convergences in probability to population mean
($\bar{X}_n \xrightarrow{P} \mu$). We will have the sample mans of an i.i.d.
sample arbitrarily close to $\mu$ if just $n$ is high enough.
%TODO: Original notes had quotes arouf last sentence. Are they needed?

\textbf{Proof: } Use Chebyshev's inequality for the complement event:
$$P(|\bar{X}_n - \mu | \geq \epsilon) \leq
\frac{\overbrace{Var(\bar{X}_n)}^{=\sigma^2/n}}{\epsilon^2} = \frac{\sigma^2}{n
\epsilon^2}$$
$$P(|\bar{X}_n - \mu | < \epsilon) = 1 - P(|\bar{X}_n - \mu | \geq \epsilon)
\geq 1 - \frac{\sigma^2}{n
\epsilon^2}$$

where $\lim_{n \to \infty} \frac{\sigma^2}{n\epsilon^2} = 0$ and  $\lim_{n \to
\infty} 1-\frac{\sigma^2}{n\epsilon^2} = 1$.

\textbf{Note} The property that the same sample quantity approaches a fixed
constant as $n \to \infty$ is called consistency (konsistenssi).

\textbf{Note} WLLN was already used in R-script \verb#demonstrateBias.R#
%TOOD: Is it meantioned in notes before?

\textbf{Note} WLLN also justifies the use of a histogram from a large number of
replicates as an approximation of the true density as $n \to \infty$. This is
because every class of the histogram gives $P(c_1 \leq x < c_1) = F(c_2) -
F(c_1).$ Weather $x$ belongs to a class $[c_1, c_2[$ is a Bernoulli($p$)
distributed random variable with
$$p = E(Y) = F(c_1) - F(c_2)$$

\textbf{Note} Another way of specifying the law of large numbers is the strong
law of strong numbers (SLLN).
$$P(\lim_{n \to \infty} \bar{X}_n = \mu) = 1.$$
$\bar{X}$ converges to $\mu$ with probability 1, meaning almost sure
convergence.
\end{theorem}
\begin{definition}
A sequence of random variables converges in distribution to random variable $X$
if
$$\lim_{n \to \infty} F_{X_n}(x) = F_X(x)$$
at all points of $x$ where $F_X(x)$ is a continuous cdf. Notation used for
convergence in distribution is $X_n \xrightarrow{d} X$.
%TODO: There is a plot a notes 6 page 6. Is it needed?
\end{definition}
\begin{example}
(Maximum of uniforms)

If $X_1, X_2, \ldots$ are i.i.d. Uniform(0,1) distributed random variables and
let $X_{(n)} = \max_{1 \leq i \leq n} X_i$. Let us explore if and to where
$X_{(n)}$ converges in distribution.

When $n \to \infty$, $X_{(n)} \to 1$ and as $X_{(n)} < 1$, we have for any
$\epsilon > 0$
$$P(|X_{(n)} - 1| \geq \epsilon) = P(1 - X_{(n)} \geq \epsilon) = P(X_{(n)}
\leq 1 - \epsilon)$$
Because sample is independent
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{lll}
P(X_{(n)} < 1 - \epsilon) & = P(\text{All } X_i \leq 1 - \epsilon) \\
& = \left[ \underbrace{P(X \leq 1 - \epsilon}_{F_X(1-\epsilon)} \right]^n & X
\sim Unif(0,1) \\
& = (1 - \epsilon)^n & \lim_{n \to \infty} (1 - \epsilon)^n = 0 \\
& \rightarrow X_{(n)} \xrightarrow{P} 1
\end{array}
\end{equation*}
% TODO: Are all needed taken from equations?
Let us take $\epsilon = \frac{t}{n}$ to rewrite
$P(X_{(n)} \leq 1 - \epsilon)$ as
$$P(X_{(n)} \leq 1 - \frac{t}{n}) = (1-\frac{t}{n})^n$$
for which $\lim_{n \to \infty} (1 - \frac{t}{n})^n = e^{-t}$.
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
X_{(n)} \leq 1 - \frac{t}{n} \\
X_{(n)} - 1 \leq - \frac{t}{n} & | \cdot (-1) \\
1 - X_{(n)} \leq \frac{t}{n} & | \cdot n \\
n(1 - X_{(n)}) \leq t \\
P(n(1 - X_{(n)}) \leq t) = e^{-t} \\
P(n(1 - X_{(n)}) \geq t) = \underbrace{1 - e^{-t}}_{\text{Exponential(1) cdf}}
\\
\end{array}
\end{equation*}
%TODO: Plot it notes 6 page 7. Is it needed?
\textbf{Illustration: } See R-script \verb#MaxOfUniforms.R#

How fast does the convergence occur, i.e. how large does the $n$ need to be to
have $n(1 - X_{(n)}) \sim Exponential(1)$?

We generate $M=10000$ samples of each of the sample sizes $n=2,3,4,5,10,20$. For
each sample, find $X_{(n)}$ and compute $y=n(1-X_(n))$. Plot the histogram if
$y$ and compare to Exponential(1) pdf.

The approximation looks quite nice already, when $n \geq 10$. But notice the
behaviour in the tails. Especially $n(1-X_{(n)})$ is bounded to interval
$[0,n]$.
%TODO: Was there some text that was left out of notes in notes 6 page 7 end?
\end{example}
\begin{theorem} (The central limit theorem (CTL) (Keskeinen raja-arvolause))

Let $X_1, X_2,\ldots$ be a sequence of i.i.d. random variables with finite
$E(X) = \mu$ and finite variance $Var(X) = \sigma^2 > 0$. Let $\bar{X}_n =
\sum_{i=1}^n X_i$. Let $G_{Z_n}(z)$ denote the cdf of
$$Z_n = \frac{\sqrt{n}(\bar{X}_n-\mu)}{\sigma} =
\frac{\bar{X}_n-\overbrace{\mu}^{E(\bar{X}_n)}}{\underbrace{\sigma/\sqrt{n}}_{sd(\bar{X}_n)}}$$
Then $\lim_{n \to \infty}G_{Z_n}{z} = \underbrace{\int_{-\infty}^z
\underbrace{\frac{1}{\sqrt{2\pi}}e^{-y^2/2}\ud y}_{N(0,1) \text{
pdf}}}_{N(0,1) \text{ cdf}}$
That is: $Z_n$ has limited standard normal distribution, meaning $Z_n
\xrightarrow{d} N(0,1)$. Equivalently, $\bar{X}
\xrightarrow{d} N(\mu,\sigma^2/n)$.
\textbf{Notes:}
\begin{itemize}
  \item We assume only finite $\mu$ and $\sigma^2$ and end up to normlity.
  \item The rate of convergence is affected by the original distribution of
  $X_i$
  \begin{itemize}
    \item The closer the distribution of $X$ is the Normal curve, the faster the
    convergence is.
    \item The rate needs to be evaluated case by case.
  \end{itemize}
  \item Provides an all purpose approximation og the distribution of sums of
  i.i.d. random variables.
\end{itemize}
\textbf{Proof: } See Casella \& Berger pages 237-238.

\textbf{Illustration: } See R-script \verb#unifmean.R#. Script takes $M=10000$
samples using each of the following sample sizes $n=1,2,3,4,5,6$ from a
Uniform(0,10) population, and demonstrates the distribution of $\bar{X}$ using a
histogram and compares it to $N(\mu, \sigma^2/n)$ distribution. Expected value
and variance of Uniform(0,10) population are
$$\mu = \frac{b-a}{2} = \frac{10}{2} = 5$$
$$\sigma^2 = \frac{1}{12}(b-a)^2 = \frac{(10-0)^2}{12} = \frac{100}{12} =
\frac{25}{3}$$
The visual evaluation of histogram gives an impression that the approximation is
good already when $n > 4$. However, we are usually interested in the behaviour
in the tails, which needs more careful evaluation, e.g. compute the
0.95\textsuperscript{th} quartile of the Normal distribution. Check which
proportion of the simulated samples had the mean above this quartile. The
proportion should be close to 0.05 if the approximation is good.
%TODO: In notes, is said that this is left as an exercise. Is this needed?
\end{theorem}
\begin{theorem}(Slutsky)
If $X_n \xrightarrow{d} X$ and if $Y_n \xrightarrow{P} a$, where a is a
constant, then
\begin{itemize}
  \item[a)] $Y_n X_n \xrightarrow{d} aX$
  \item[b)] $X_n + Y_n \xrightarrow{d} X + a$
\end{itemize}
\textbf{Note} Proof for this is not shown.
\end{theorem}
\begin{example}
(Normal approximation with estimated variance)

The central limit theorem said that under mild conditions
$$\frac{\sqrt{n}(\bar{X}_n-\mu)}{\sigma} \xrightarrow{d} N(0,1)$$
However, $\sigma^2$ is often unknown, but can be unbiasedly and consistently
estimated by $s_n^2 = \frac{1}{n-1}\sum(X_i-\bar{X}_n)^2$. It can also be shown
that (see Casella \& Berger example 5.5.3 and exercise  3.32)
$$\frac{\sigma}{s_n} \xrightarrow{P} 1$$
\textbf{Note}
$$\frac{\sqrt{n}(\bar{X}_n-\mu)}{s_n} =
\underbrace{\frac{\sigma^2}{s_n}}_{\frac{\sigma}{s_n} \xrightarrow{P}
1}\underbrace{\frac{\sqrt{n}(\bar{X}_n-\mu)}{\sigma^2}}_{\frac{\sqrt{n}(\bar{X}_n-\mu)}{\sigma}
\xrightarrow{d} N(0,1)} \xrightarrow{d} N(0,1)$$
So even tough the variance is estimated, $\frac{\bar X - \mu}{s / \sqrt{n}}$
converges to a normal distribution. However, intuitively, the convergence should
be more slow than when $\sigma$ is known.
% TODO: There was also mention that this was left as an exercise. Is is still
% exercise?
\end{example}
\chapter{Estimation}
In estimation, we want to use sample $X_1, \ldots, X_n$ to make inference on an
unknown population parameter $\theta$. It means that we want to find a statistic
$T(\bm X)$ that somewhat optimally captures the information of the sample on the
parameter of interest, $\theta$.

Most commonly. $\theta$ is the population mean, but it can also be related to
population variance,covariance, median, maximum ect. Very often the population
parameters, especially the mean, is not a scalar number, but it's function of
some known characteristics of sampled units $i$, and therefore $\theta$ includes
parameters of that function.

\begin{example} Simple linear regression

%TODO: Is this example good? Wrote something based on the stuff given in notes.
%TODO: Plot in notes 7 page 4 left out. Is it needed?
Consider case of simple linear regression, where $X$ is a fixed known
predictor, and $Y$ is a random variable we want to predict with values of $X$.
The model can be of following for
$$Y_i = \mu(X_i) + e_i$$
Where $\mu(X_i)$ is a fixed mean of $Y$
$$\mu_i = \mu(X_i) = \beta_0 + \beta_1 X_i$$
and $e_i$ is the random error related to value of $Y_i$
$$e_i \sim N(0, \sigma^2)$$
This would mean that distribution of $Y_i$ would be
$$Y_i \sim N(\mu(x_i), \sigma^2)$$
In this case, we are interested in estimating the the regression coefficients
$$\bm \theta = 
\begin{bmatrix}
\beta_0 \\ \beta_1
\end{bmatrix}$$
\end{example}
\begin{example}
Lets consider a random variable
\begin{equation*}
\renewcommand{\arraystretch}{1.3}
Y_i = \left\{
\begin{array}{rl}
1 & \text{if tree $i$ is dead} \\
0 & \text{if tree $i$ is alive}
\end{array} \right.
\end{equation*}
Parameter of interest might be the probability of a tree being dead.
$$Y_i \sim Bernoulli(p), ~ \theta = p$$
If we also know the age of the tree, and it is justified to assume that older
trees are more commonly dead than younger trees, then
$$Y_i \sim Bernoulli(p_i)$$
$$p_i = \beta_0 + \beta_1 \text{Age}_i, ~ 
\rightarrow \bm \theta = 
\begin{bmatrix}
\beta_0 \\ \beta_1
\end{bmatrix}$$
%TODO: Left out plot in notes 7 page 4. is it needed?
Because $\beta_0 + \beta_1 \text{Age}_i$ is not restricted to
$\left[0,1\right]$, a better model uses some function for $p(\text{Age}_i)$.
Most commonly we use
$$log\left(\frac{p_i}{1-p_i}\right) = \beta_0 + \beta_1 \text{Age}_i$$
Which is a logit transformation $[0,1] \rightarrow \ureal$. This provides us
with logistic regression.
\textbf{Note:} See also example 1.2 and 1.3 in \verb#notes.pdf#
\end{example}
\section{Two principles}
A sufficient statistic (tyhjentävä statistiikka) for a population parameter
$\theta$ is such statistic $T(\bm X)$, that in certain sense it captures all
the information about $\theta$ contained in the sample.
%TODO: Figure out if these sould be sections of definitions or other.
\section{The sufficient principle}
If $T(\bm X)$ is a sufficient statistic for $\theta$, then any inference about
$\theta$ should depend on $\bm X$ only through the value of $T(\bm X)$.
\begin{definition} A statistic $T(\bm X)$ is sufficient statistic for $\theta$
if the conditional distribution of the sample given $T(\bm X) [\bm X | T(\bm
X)]$ does not depend on $\theta$.
\end{definition}
\begin{example}
Let $X_1,\ldots,X_n$ be i.i.d. from $N(\mu, \sigma^2)$ population, where
$\sigma^2$ is known. Now the sample mean $\bar X$ is a sufficient statistic for
$\mu$.

That is, if we have the complete data and you know only the sample mean, we both
still have all information on $\mu$ the sample includes. However, if a third
person knows only $min(\bm X)$, $max(\bm X)$ and $median(\bm X)$, he still does
not have much information on $\mu$ as you have when you know $\bar X$.

If also $\sigma^2$ is unknown, then
$\bm T(\bm X) = \begin{bmatrix}
\bar X \\ s^2
\end{bmatrix}$ is sufficient for $\bm \theta = \begin{bmatrix}
\mu \\ \sigma^2
\end{bmatrix}$

For more formal discussion on the topic, see Casella \& Berger section 6.2.
\end{example}

\section{The likelihood function}
\begin{definition} 
\label{defLikelihood1}
Let $f(\bm x, \bm \theta)$ denote the joint pdf or pmf of
the sample $\bm X = X_1, \ldots, X_n$. Then Given that $\bm X = \bm x$ has been
observed, the function of $\theta$ defined by
$$L(\bm \theta | \bm x) = f(\bm x | \theta)$$
is called the likelihood function (uskottavuusfunktio).

\textbf{Note }We do not assume $X_1,\ldots,X_n$ to be independent.
\end{definition}
\begin{itemize}
  \item If $\bm X$ is a discrete random vector, then
  $$L(\bm \theta | \bm X) = P_\theta(\bm X = \bm x)$$
  \item If we compare the likelihood at two parameter points $\theta = \theta_1$
  and $\theta = \theta_2$, and find out that
  $$P_{\theta_1}(\bm X = \bm x) = L(\theta_1 | \bm x) > L(\theta_2 | \bm
  x) = P_{\theta_2}(\bm X = \bm x)$$
  then the sample we observed is more likely to have occurred when $\theta =
  \theta_1$ than when $\theta =\theta_2$.
  \begin{itemize}
    \item[$\rightarrow$] $\theta_1$ is more plausible (uskottava) value for
    $\theta$ than $\theta_2$.
  \end{itemize}
  \item If $X$ is a continuous, real valued random variable, then for a small
  value $\epsilon$
  $$P_\theta(X - \epsilon < X < X + \epsilon) \approx 2 \epsilon f(X, \theta)
  = 2 \epsilon L(\theta | X)$$
  %TODO: Left out plot and text related to it on notes 8 page 2. Are they
  % needed?
  therefore
  $$LR = \frac{P_{\theta_1}(X - \epsilon < X < X + \epsilon)}{P_{\theta_2}(X -
  \epsilon < X < X + \epsilon)} \approx \frac{L(\theta_1 | X)}{L(\theta_2 | X)}$$
  \begin{itemize}
    \item[$\rightarrow$] If the $LR > 1$, then the sample was more likely to
    occur when $\theta = \theta_1$, and if $LR < 1$, then the sample was more
    likely to occur when $\theta = \theta_2$.
  \end{itemize}
\end{itemize}
\begin{example}
Let $X$ have negative binomial distribution. The Negative binomial distribution
specifies the pmf for the number of successes before a fixed number of a
failures ($r$) in a series of Bernoulli trials with success probability $p$. The
pmf
$$P(X=x |r,p) = {{r+x-1}\choose x} p^r (1-p)^x, ~ \text{where } x=0,1,\ldots
\text{ and } 0 \leq p \leq 1$$
Consider an experiment where $r=3$ and it was observed that $X=2$. The
likelihood as a function of $\theta = p$
$$L(p | 2) = P_p(X=2) = {4 \choose 2}p^3(1-p)^2$$
\begin{itemize}
    \item[$\rightarrow$] The likelihood is a 5\textsuperscript{th} order
    polynomial with respect to $p$.
\end{itemize}
In general, if $X=x$ has been observed,
$$L(p | x) = P_p(X=x) = {3+x-1 \choose 2}p^3(1-p)^x$$
in which case the likelihood is a polynomial of order $x+3$.
\end{example}
\begin{example} 
\label{exampleGropedData1}
Let $\bm X_{3 \times 1}$ have 3-variate normal distribution so
that all components have a common, unknown mean of $\mu$ and a known, common
variance $\sigma^2 = 3.69$. In addition, $cov(X_1, X_2)=2.25$ and $cov(X_1,
X_3) = cov(X_2, X_3) = 0$.

This kind of situation could stem from a grouped structure of the data: $X_1$
and $X_2$ might belong to the same group (e.g. school class) and $X_3$ may
originate from another group. That is
$$\bm X \sim N(\bm \mu, \bm \Sigma) \text{, where } \mu = \begin{bmatrix}
\mu \\ \mu \\ \mu
\end{bmatrix} \text{ and }
\bm \Sigma = \begin{bmatrix}
3.69 & 2.25 & 0 \\
2.25 & 3.69 & 0 \\
0 & 0 & 3.69 
\end{bmatrix}$$
The likelihood of this model for observation $\bm x = \begin{bmatrix}
6 \\ 5 \\ 4
\end{bmatrix}$ has been illustrated in R-script \verb#MVNlikelihood.R#.

\textbf{Note } The likelihood function is not a pdf as a function of $\theta$.
Therefore we say, that $\theta_1$ is more plausible than $\theta_2$ (not more
probable). Note also, that $\theta$ is thought to be fixed, therefore it makes
no sense to specify a pdf for it. (In Bayesian statistics, $\theta$ is thought to
be random.)
\end{example}

%TODO: Should following be a definition or something rather than a section?
\section{The likelihood principle}
If $\bm x$ and $\bm y$ are two observed sample points such that $L(\theta |
\bm x)$ is proportional to $L(\theta | \bm y)$, that is there exists a constant
$C(\bm x, \bm y)$ such that
$$L(\theta | \bm x) = C(\bm x, \bm y)L(\theta | \bm y)~ \text{ for all }
\theta$$
then the conclusions drawn from $\bm x$ and $\bm y$ should be identical.
\textbf{Note} $C(\bm x, \bm y)$ may be different for different $(\bm x, \bm y)$
pairs, but does not dependent on $\theta$.

\section{Point estimation}
\begin{definition}
A point estimator is any function $W(X_1,\ldots,X_n)$. That is, any statistic is
a point estimator.
\end{definition}
\begin{itemize}
  \item We now first consider the methods to find estimators and there after
  consider criteria to evaluate, whether the estimators are good or bad.
  \item[\textbf{Note}] Estimator is a function of the sample $X_1,\ldots,X_n$
  and estimate is the numerical value the estimator got after the sample was
  actually taken.
  \begin{itemize}
    \item[$\rightarrow$] Estimator is a random variable, and estimate is its
    realized value in the sample.
  \end{itemize}
  \item Often, there is a natural, intuitive candidate fro a point estimator,
  e.g. $\bar X$ is a intuitive choice for a point estimator of $\mu$.
\end{itemize}
%TODO: In notes, this was a subsection (3.2.1). Should it be here?
\section{Estimating the population mean using least squares}
Based on theorem \ref{theoremXbarSigma}, the ordinary least squares estimator of
parameter $\mu$ using observed sample values $y_1, \ldots, y_n$ is found by
$$min_\theta \sum_{i=1}^n (y_i-\theta)^2$$
The solution is $\theta = \bar y$ %TODO: In notes was \bar X.
The approach also generalized to cases where $E(Y_i)$ is a function of a fixed
variable $x_i$. Consider a model $Y_i = \beta_0 + \beta_1 x_i + \epsilon_i$,
where $Y_i$ is a random variable and $x_i$ is another fixed observable variable.
We can $\beta_0$ %TODO: How was this sentense suposed to be formed?
such that $E(\epsilon_i) = 0$ and 
$$E(Y_i) = E(\underbrace{\beta_0 + \beta_1 x_i}_{\text{fixed}} +
\underbrace{\epsilon_i}_{E(\epsilon_i) = 0}) = \beta_0 + \beta_1 x_i = E(Y_i
| x_i) = \mu(x_i, \bm \beta)$$
Parameters $\bm \beta$ are ``second order'' parameters. They quantify the
(assumed) linear relationship between $x_i$ and $E(Y_i)$. Using the idea of
theorem \ref{theoremXbarSigma}, $\bm \beta$ can be estimated using
$$min_{\beta_0, \beta_1} \sum_{i=1}^n (y_i-(\beta_0 + \beta_1 x_i))^2$$
The solutions are
$$\widehat{\beta_1} = \frac{\sum(x_i - \bar x)(y_i - \bar y)}{\sum(y_i -
\bar y)^2} \text{ and } \widehat{\beta_0} = \bar y - \beta_1 \bar x$$

\begin{itemize}
  \item A more general formulation results by defining
  $$\bm X = \begin{bmatrix} 1 & X_1 \\ \vdots & \vdots \\ 1 & X_n
  \end{bmatrix}, ~ 
  \bm Y = \begin{bmatrix} Y_1 \\ \vdots \\ Y_n \end{bmatrix}, ~
  \bm \beta = \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix}, ~
  \bm \epsilon = \begin{bmatrix} \epsilon_1 \\ \vdots \\ \epsilon_n
  \end{bmatrix}, ~$$ 
  and then the model is $\bm Y = \bm X'\bm \beta + \bm \epsilon$
  \item The least squares estimator of $\bm \beta$ is found by minimizing
  $$\bm \epsilon' \bm \epsilon = (\bm y - \bm X' \bm \beta)'(\bm y - \bm X' \bm
  \beta)$$
  \item The solution is $\widehat{\bm \beta} = (\bm X' \bm X)^{-1}\bm X' \bm y$
  \item This is a general solution for the multiple regression case as well.
\end{itemize}
The least squares method is optimal and intuitive way to find the estimators for
population mean. However, it does not provide tools to estimate other parameters
of distribution. For that purpose, we will next present three other approaches 
for parameter estimation in general case:
\begin{itemize}
  \item The method of moments
  \item Maximum likelihood
  \item Bayesian approach
\end{itemize}
\section{The method of moments}
\begin{itemize}
  \item This method dates back to late 1800's.
\end{itemize}
Let $X_1,\ldots,X_n$ be a sample from population with pmf or pdf $f(x |
\theta_1,\ldots,\theta_k)$. The method of moments (momenttimenetelmä), moment
matching, finds estimates of $\theta_1,\ldots,\theta_k$ by equating the first
$k$ sample moments to the corresponding population moments. \\

\textbf{Define}
\begin{equation*}
\renewcommand{\arraystretch}{1.7}
\begin{array}{ll}
m_1 = \frac{1}{n}\sum_{i=1}^n x_i & \mu_1' = E(X^1)\\
m_2 = \frac{1}{n}\sum_{i=1}^n x_i^2 & \mu_2' = E(X^2)\\
\vdots & \vdots \\
m_k = \frac{1}{n}\sum_{i=1}^n x_i^k & \mu_k' = E(X^k)\\
\end{array}
\end{equation*}
\begin{itemize}
  \item The population moments will typically be functions of $\bm \theta$,
$\mu_j'(\theta_1, \ldots, \theta_k)$
	\item The moment estimator $(\tilde{\theta}_1,\ldots,\tilde{\theta}_k)$ of
	$(\theta_1, \ldots, \theta_k)$ is found by solving the following system of
	equations for $(\theta_1, \ldots, \theta_k)$
	\begin{equation*}
	\renewcommand{\arraystretch}{1.3}
	\left\{
	\begin{array}{l}
	m_1 = \mu_1'(\bm \theta) \\
	m_2 = \mu_2'(\bm \theta) \\
	\vdots \\
	m_k = \mu_k'(\bm \theta) \\
	\end{array} \right.
	\end{equation*}
\end{itemize}
\begin{example}
Let $X_1,\ldots,X_n$ be i.i.d. with $X_i \sim N(\mu, \sigma^2)$, and both $\mu$
and $\sigma^2$ are unknown.
\begin{equation*}
\renewcommand{\arraystretch}{1.3}
\begin{array}{ll}
m_1 = \bar X & m_2 = \frac{1}{n}\sum_{i=1}^n X_i^2 \\
\mu_1' = \mu & \mu_2' = E(X^2) = \mu^2 + \sigma^2
\end{array}
\end{equation*}
\textbf{Recall} $Var(X) = E(X^2) - (E(X))^2 \rightarrow E(X^2) = Var(X) +
(E(X))^2 = \sigma^2 + \mu^2$
Following system of equation needs to be solved for $\mu$ and $\sigma^2$
\begin{equation*}
\renewcommand{\arraystretch}{1.7}
\begin{array}{l}
\left\{
\begin{array}{l}
\bar X = \mu \\
\frac{1}{n}\sum X^2 = \mu^2 + \sigma^2 \\
\end{array} \right. \\ \\
\left\{
\begin{array}{l}
\bar X - \mu = 0 \\
\frac{1}{n}\sum X^2 - \mu^2 - \sigma^2 = 0 \\
\end{array} \right. \\
\end{array}
\end{equation*}
We get 
$$\tilde \mu = \bar X$$
$$\tilde \sigma^2 = \underbrace{\frac{1}{n}\sum(X_i - \bar X)}_{\text{Biased
estimator}} \neq \underbrace{\frac{1}{n-1}\sum(X_i - \bar X)^2}_{\text{Unbiased
estimator}}$$
But the bias vanishes as $n \to \infty$.

This example was easy, since the system of equations we needed to solve was
linear.
\end{example}
\begin{example} A numerical solution
\label{exampleMomentsWeibull1}
Let $X_1,\ldots,X_n$ be i.i.d. from the Weibull($a$,$b$) distribution, which has
the pdf
$$f(x) = \frac{a}{b}\left(\frac{x}{b}\right)^{a-1}e^{-(x/b)^a} ~ \text{ where
} 0 < x < \infty \text{ and } a,b > 0$$
where $a$ is called te shape parameter and $b$ is called the scale parameter.

The first two moments are
$$E(X) = \int_0^\infty f(x)x \ud x = \ldots = b
\Gamma\left(1+\frac{1}{a}\right)$$
$$E(X^2) = \int_0^\infty f(x)x^2 \ud x = \ldots = b^2
\Gamma\left(1+\frac{2}{a}\right)$$
where $\Gamma(z) = \int_0^\infty z^{u-1}e^{-z}\ud z$ cannot be written in closed
form, but can be well approximated using function gamma in R.

The moment-matching system of equations is
\begin{equation*}
\renewcommand{\arraystretch}{1.7}
\left\{
\begin{array}{l}
\frac{1}{n} \sum x_i - b
\Gamma\left(1+\frac{1}{a}\right) = 0 \\
\frac{1}{n}\sum X^2 - b^2
\Gamma\left(1+\frac{2}{a}\right) = 0 \\
\end{array} \right. \\
\end{equation*}
This is nonlinear with respect to $a$!

We solve it for $a$ and $b$ using the Gauss-Newton method, which is a
multivariate generalization of the Newton-Raphson method. Example in R-script
\verb#WeibullMoment.R# generates first a random sample of size 30 from
Weibull(2,10) population. Using the generalized sample, it finds the moment
estimates of $a$, $b$ numerically using R-function NRnum.

The observed sample moments are
$$m_1 = \bar X = 9.604, ~ m_2 = \frac{1}{n}\sum x^2 = 113.22$$
We solve
\begin{equation*}
\renewcommand{\arraystretch}{1.7}
\left\{
\begin{array}{l}
9.604 - b
\Gamma\left(1+\frac{1}{a}\right) = 0 \\
113.22 - b^2
\Gamma\left(1+\frac{2}{a}\right) = 0 \\
\end{array} \right. \\
\end{equation*}
for $(a,b)$ ($\bm \theta = \begin{bmatrix}a & b\end{bmatrix}^T$). We use $a'=1$
and $b'=9.60$ as starting values, and we get
$$\tilde a = 2.21 ~ \tilde b = 10.84$$
\end{example}
\section{Newton-Raphson}
Consider function $f(a) = 0$, which needs to be solved for $a$.
%TODO: Left out a plot on notes 9, page 5
\begin{itemize}
  \item[1.] Start with some guess $a'$, compute $f(a')$ and $f'(a')$ to
  approximate $f(a)$ by a linear function. Find $a$ such that the approximator
  = 0.
   \item[2.] Set the solution as $a'$ and repeat step 1.
\end{itemize}
If the initial guess $a'$ was good enough and $f$ is continuous and
differentiable, we will find the approximate solution within few iteration.

Gauss-Newton generalized this to vector valued $\bm a$ and $\bm x$. The
algorithm has been implemented in R-function NRnum of package lmfor. The
function evaluates also the required derivatives $\frac{\ud f(\bm x)}{\ud
\theta_i}$ numerically.
\section{Maximum likelihood}
Definition \ref{defLikelihood1} defined the likelihood as
$$L(\bm \theta | \bm x) = f(\bm x | \theta)$$
where
$$\bm \theta = \begin{bmatrix}
\theta_1 \\ \vdots \\ \theta_k
\end{bmatrix}$$
\textbf{Note} For i.i.d. data $X_1, \ldots, X_n$ from population $f(x |
\theta)$, the likelihood simplifies to
$$L(\bm \theta | \bm x) = \prod_{i=1}^n f(\bm x | \theta)$$
\begin{definition}
For each sample point $\bm x$, let $\hat{\bm \theta}(\bm x)$ be a parameter
value at which $L(\bm \theta | \bm x)$ attains its maximum value as a function
of $\bm \theta$, with $\bm x$ fixed. A maximum likelihood estimator of $\bm
\theta$ based on sample $\bm x$ is this function $\hat{\bm \theta}(\bm x)$. The
maximum likelihood estimate (ML estimate) is the numerical value of that
function in the data point we have.
%TODO: Are all the x's capitals or lower case? (or are some capitals and some
% lower case?)

\textbf{Note} We use abbreviation MLE for both ML estimator and ML estimate.
\end{definition}
\begin{itemize}
  \item MLE is the parameter point for which the observed sample is most likely.
  \item The MLE also fulfills some optimality criteria, which will be discussed
  later.
  \item How to find MLE?
  \begin{itemize}
    \item If $L(\bm \theta | \bm x)$ is differentiable with respect to  all
    $\theta_i$, then the candidates for MLE are the values that solve
    $$\frac{\ud}{\ud \theta_i} L(\bm \theta | \bm x) = 0, ~ i=1,\ldots,k$$
    \item These together with the boundaries of the parameter space provide
    candidates of MLE.
    \item Of the candidates the one that gives the maximum
    value of $L(\bm \theta | \bm x)$ is the MLE.
  \end{itemize}
\end{itemize}
\begin{example} Example ML1
\label{exampleML1}

Let $X_1,\ldots,X_n$ be i.i.d. from $N(\theta,1)$, meaning only the population
mean $\theta$ is unknown.
\begin{equation*}
\renewcommand{\arraystretch}{1.7}
\begin{array}{rl}
L(\theta, \bm x) & = \prod_{i=1}^n \frac{1}{(2\pi)^{1/2}}e^{-\frac{1}{2}(x_i -
\theta)^2} \\
& = \frac{1}{(2\pi)^{n/2}}e^{-\frac{1}{2}\sum_{i=1}^n(x_i -
\theta)^2} \\
\frac{\ud}{\ud \theta}L(\theta | \bm x) & = \frac{1}{(2\pi)^{n/2}}e^{-\frac{1}{2}\sum_{i=1}^n(x_i -
\theta)^2}(-\frac{1}{2})\sum_{i=1}^n(-2(x_i - \theta)) \\
& = L(\theta | \bm x) \sum_{i=1}^n(x_i-\theta)
\end{array}
\end{equation*}
$\frac{\ud}{\ud \theta}L(\theta | \bm x) = 0$ if
\begin{equation*}
\renewcommand{\arraystretch}{1.7}
\begin{array}{lll}
\text{either } & L(\theta | \bm x) = 0 & \text{A} \\
\text{or} & \sum_{i=1}^n(x_i - \theta) = 0 & \text{B}
\end{array}
\end{equation*}
But if $L(\theta | \bm x) = f(\bm x | \theta) > 0$
\begin{itemize}
    \item[$\to$] There is no solution for A.
\end{itemize}
B gives
\begin{equation*}
\renewcommand{\arraystretch}{1.7}
\begin{array}{l}
\sum(x_i - \theta) = 0 \\
\sum x_i - \sum \theta = 0 \\
\sum x_i - n \theta = 0 <=> \hat \theta = \frac{\sum x_i}{n} = \bar x
\end{array}
\end{equation*}
So the sample mean, which is the least squares and moment estimator for the
population mean is also the ML estimator for the population mean.

To confirm that $\bar X$ is the MLE, we still ned to make sure that
\begin{itemize}
  \item[1)] $\bar x$ is the maximum, not minimum
  \item[2)] The boundaries of the support of $\theta$ do not provide higher
  value of $\theta$
\end{itemize}
We can conclude that
\begin{itemize}
  \item[1)] This is maximum because
  $$\frac{\ud^2}{\ud \theta^2} L(\theta | \bm x)_{\theta=\bar x} < 0$$
  \item[2)] We can notice that
  $$\lim_{\theta \to \infty} L(\theta | \bm x) = \lim_{\theta \to
  -\infty}L(\theta | \bm x) = 0$$
  and $L(\bar x | \bm x) > 0$.
\end{itemize}
%TODO: Left out two plots at end of notes 9 page 8. Are they needed?
\end{example}
%TODO: Left out notes 10 page 1. Is it needed?
\begin{itemize}
\item In most cases, the solution of the estimation problem becomes easier if
the natural logarithm of the likelihood is used instead of likelihood.
\item The maximum of $L(\bm \theta | \bm x)$ and $ln(L(\bm \theta | \bm x))$
coincide (are at the same point $\bm \theta$) because $ln$-function is
increasing on $(0, \infty)$.
\end{itemize}
%TODO: Left out plot at notes 10 page 2. Is it needed?
\begin{example}
Example ML2 (Bernoulli MLE)

Let $X_1,\ldots,X_n ~ Bernoulli(p)$, i.i.d.
$$L(p | \bm x) = \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i} = (*)$$
Recall that
\begin{equation*}
\renewcommand{\arraystretch}{1.7}
\text{PMF}: f(x_i | p) = \left\{
\begin{array}{ll}
p & \text{if } x_i = 1 \text{ (success)} \\
1-p & \text{if } x_i = 0 \text{ (failure)} \\
\end{array} \right. \\
\end{equation*}

$$(*) = p^{\sum x_i}(1-p)^{\sum(1-x_i)} = p^{\sum x_i}(1-p)^{n-\sum x_i}$$
The log-likelihood is
$$ln(L(p | \bm x)) = ln(p^{\sum x_i}(1-p)^{n-\sum x_i}) = \sum x_i ln(p) +
(n-\sum x_i)ln(1-p)$$
$$\frac{\ud}{\ud p}ln(L) = \sum x_i \frac{1}{p}+(n-\sum x_i)\frac{-1}{1-p} =
\ldots = \frac{\sum x_i - np}{p(1-p)}$$

\begin{equation*}
\renewcommand{\arraystretch}{1.7}
\begin{array}{ll}
\frac{\ud}{\ud p}ln(L) = 0 \\
\frac{\sum x_i - np}{p(1-p)} = 0 & || \cdot p(1-p) \neq 0 \\
\sum x_i - np = 0 \\
p = \frac{\sum x_i}{n} = \bar x
\end{array}
\end{equation*}
Check that this is a global maximum and $ln(L(\bar x)) > ln(L(0))$ and
$ln(L(\bar x)) > ln(L(1))$ to confirm that the solution is also the MLE.
\end{example}
\begin{example}
Example ML3 (MLE in dependent sample)

Recall the grouped data example \ref{exampleGropedData1}, where $Y \sim N(\bm
\mu, \bm \Sigma)$. Assume that $Y$ has been observed to be $\bm y =
\begin{bmatrix}6 \\ 5 \\ 4\end{bmatrix} ~(\bar y = 5)$. Assume (unrealistically)
that $\bm \Sigma$ is known to be
$$\bm \Sigma = \begin{bmatrix}
3.69 & 2.25 & 0 \\
2.25 & 3.69 & 0 \\
0 & 0 & 3.69 
\end{bmatrix}$$
We want to estimate parameter $\theta$ of the unknown vector $\bm \mu =
\begin{bmatrix}\theta \\ \theta \\ \theta \end{bmatrix}$ (i.e. we assume that
all three components of $\bm \mu$ have the common mean of $\theta$). The
likelihood is directly the pdf of the multivariate normal distribution and the
log-likelihood is the natural logarithm of that.

R-script \verb#MVNMLE.R# does the estimation. Define the likelihood as we did
previously in the R-script \verb#MVNLikelihood.R#. Find the value of $\theta$
that maximizes the likelihood.
%TODO: Left out plot in notes 10 page 4. Is it needed?

\textbf{1.} Simple solution (for demonstration). Compute $L(\theta)$ for values
$\theta=(0, 0.001, 0.002,\ldots,10)$ and select the one that provides the
highest value.  This direct search or grid search algorithm have the estimate 
$\hat \theta = 4.831 < \bar y = 5$, because the independent $Y_3 = 4$ gets more
weight in estimation than the two mutually dependent elements $\begin{bmatrix}
Y_1  \\ Y_2 \end{bmatrix} = \begin{bmatrix} 6 \\ 5 \end{bmatrix}$. Therefore 
$\hat \theta$ is shrunken towards $Y_1 = 4$ compared to $\bar y = 5$.

\textbf{2.} We solved this also using function mle of R-package stats4.
\begin{itemize}
  \item[1.] Define a R-function that evaluates the negative log-likelihood
  $-ln(L)$; call it e.g. nll.
  \item[2.] Specify some good starting values of the parameters in a named list.
  \item[3.] Make a call to mle: \verb#mle(nll, start=list(theta=5))#
  $\to$ gives $\hat \theta = 4.831081$.
\end{itemize}
\textbf{Note} To get more stable and faster solution, you could also provide
analytical derivative of the function you used

\textbf{Note} mle gives only the candidate of MLE, so you are responsible for
checking that it really is the MLE.
\end{example}

\begin{itemize}
  \item If $\bm \theta = (\theta_1, \theta_2,\ldots,\theta_k)$ is
  multidimensional, then we need to maximize the $ln(L)$ with respect to all
  components $\theta_i$ simultaneously.
  \item If the log-likelihood is differentiable with respect to all $\theta_i$,
  we hust need to find the practical derivatives of $ln(L)$ (or L) with respect
  to all $\theta_i$ and set them to 0
  \begin{itemize}
    \item[$\to$] We get system of $k$ equations.
  \end{itemize}
\end{itemize}
\begin{example} Example ML4
Let $X_1,X_2,\ldots,X_n$ be i.i.d. form $N(\theta, \sigma^2)$ population, and
let both $\theta$ and $\sigma^2$ be unknown. Now
$$L(\theta, \sigma^2 | \bm x) = \frac{1}{(2\pi
\sigma^2)}^{n/2}e^{\frac{1}{2}\sum(x_i - \theta)^2/\sigma^2}$$
$$ln(L(\theta, \sigma^2 | \bm x)) = -\frac{n}{2}ln(2\pi) -
\frac{n}{2}ln(\sigma^2) - \frac{1}{2}\frac{\sum(x_i-\theta)^2}{\sigma^2}$$
The ML equations becomes
\begin{equation*}
\renewcommand{\arraystretch}{1.7}
\left\{
\begin{array}{l}
\frac{\ud}{\ud \theta}ln(L(\theta, \sigma^2 | \bm x)) =
\frac{1}{\sigma^2}\sum(x_i - \theta)\\
\frac{\ud}{\ud \sigma^2}ln(L(\theta, \sigma^2 | \bm x)) =
-\frac{n}{2\sigma^2}+\frac{1}{2\sigma^4}\sum(x_i - \theta)^2 \end{array} \right.
\\
\end{equation*}
Setting these to 0 and solving for $\bm \theta = \begin{bmatrix}
\theta \\ \sigma^2 \end{bmatrix}$ gives 
$$\hat \theta = \bar x \text{ and } \sigma^2 = \frac{1}{n}\sum(x_i-\bar x)^2$$
These are the candidates for the MLE, but it can be shown that they are also the
MLE's. See Casella \& Berger p.322 for more formal argumentation.
\end{example}
The so called invariance property of MLE's is highly useful in making
restrictions to the parameters. Suppose that the distribution is indexed using
$\theta$, but we want to estimated a function of it, $\tau(\theta)$.

\begin{theorem}
If $\hat\theta$ is the MLE of $\theta$, and $\tau(\theta)$ is any function of
$\theta$, then $\tau(\hat \theta)$ is the MLE of $\tau(\theta)$.

\textbf{Proof:} See Casella \& Berger, p.320.
\end{theorem}
\begin{example} Example ML5

Recall the Weibull population from the example \ref{exampleMomentsWeibull1}
for which we estimated the parameters $a$ and $b$ using the method of moments.
Because $a$ and $b$ are both positive, it is convenient to reparametrize the pdf
in terms of $ln(a)$ and $ln(b)$.

Weibull MLE: $\bm x = (14.7,\ldots,17.56)$

The MLE's of $\bm \theta = (a, b)$ is
\begin{table}[h]
\caption{The MLE's of $\bm \theta = (a, b)$.}
\label{MLEWeibullTable1}
\begin{tabular}{| r | r | r |}
\hline
{ } & In the original scale & $exp(\widehat{ln(\theta)})$ \\
\hline
$\hat a$ &  2.24 & 2.24  \\
\hline
$\hat b$ & 10.88 & 10.88 \\
\hline
\end{tabular}
\end{table}

In R-script \verb#WeibullMLEInvariance.R# $A = ln(a)$ and $B = ln(b)$.

\begin{equation*}
\renewcommand{\arraystretch}{1.7}
\begin{array}{ll}
\widehat{ln(a)} = \hat A = 0.81 & exp(\hat A) = 2.243 \\
\widehat{ln(b)} = \hat B = 2.89 & exp(\hat B) = 10.876
\end{array}
\end{equation*}
\end{example}
\begin{example} Example ML6 (MLE in dependent data)

Recall exercise \ref{exampleSpatialData1} where we had spatial, multivariate
normal data. Consider observations of $Y_i$ in the following 2-dimensional grid,
$i=1,\ldots,25$.
%TODO: Left out grid in notes 11 page 3. It's probably needed?
$\bm Y = \begin{bmatrix} Y_1 \\ \vdots \\ Y_25 \end{bmatrix}$
is generated by following model
$$Y \sim N(\bm \mu, \bm \Sigma), \text{ where } 
\bm \mu = \begin{bmatrix} \mu_1 \\ \vdots \\ \mu_{25} \end{bmatrix},
\mu_i = 0 \text{ for all }i \text{, and } Var(Y_i) = \simga_i^2 = 2^2 \text{
for all }i$$
$$corr(Y_i, Y_j) = exp^{-\phi s_ij} \text{, where } \phi = \frac{1}{2}$$
and $s_ij$ is the euclidian distance between the location $i$ and $j$,
$s_ij = ||u_i - u_j||$.

Let us first generate the data and illustrate is graphically; see R-script
\verb#MLESpatial.R#. Next, given the simulated data, let us estimate the
parameters $\mu$ (common mean for all $i$), $\sigma^2$ (common variance for all
$i$) and $\phi$ (the parameter of correlation function).

Because $\sigma^2 > 0$ and $\phi > 0$, we parametrize the likelihood function in
terms of $ln(\sigma^2)$ and $ln(\phi)$, and define the negative log-likelihood
in R-function nll. The script provides following resulting estimates.

\begin{equation*}
\renewcommand{\arraystretch}{1.7}
\begin{array}{ll}
\hat \mu = -1.87 & \text{True: } 0 \\
\widehat{ln(\phi)} = -2.03 \to \hat \phi = 0.13 & \text{True: } 0.5 \\
\widehat{ln(\sigma^2)} = 2.35 \to \hat \sigma^2 = 10.47 & \text{True: } 4 \\
\end{array}
\end{equation*}
\end{example}

\section{Bayes estimators}

In the Bayesian approach, the fundamental difference to the previous methods is
to think the parameters $\bm \theta$ as random variable (vector), not as fixed
quantity.

\begin{itemize}
  \item The variability of $\bm \theta$ in the population is described by some
  distribution called the prior distribution.
  \begin{itemize}
    \item It is the belief of the experimenter about $\bm \theta$, and is
    formulated before the data are seen.
    \item This prior is updated with the data $\bm x$ to get the posterior
    distribution of $\bm \theta$
    %TODO: Left out plot in notes 11 page 4. Is it needed?
  \end{itemize}
  \item There are two main reasons to use Bayesian approach:
  \begin{itemize}
    \item[1.] We may be willing to combine the information we already have on
    $\theta$ with the new information we get from the data.
    \item[2.] We do not actually believe on the prior, but we use the Bayesian
    approach because it provides tools for estimation in the situation where
    e.g. ML is very tricky.
    \begin{itemize}
    \item[-] Especially, the Bayesian computational methods provide
    estimation tools for such cases.
  	\end{itemize}
  \end{itemize}
  \item Let $\pi(\bm \theta)$ be the prior pdf and let $f(\bm x | \bm \theta)$
  be the sampling pdf of the data $\bm x$.
  \begin{itemize}
    \item The posterior distribution is the conditional distribution of $\bm
    \theta$ given the sample $\bm x$
    $$\pi(\bm \theta | \bm x) = \frac{f(\bm x | \bm \theta)\pi(\bm
    \theta)}{m(\bm x)}$$
    where $m(\bm x) = \int f(\bm x | \bm \theta) \pi(\bm \theta) \ud \theta $
   \end{itemize}
   \textbf{Note} $\theta$ means both the random variable and it's realized value
   here. %TODO: was \theta suposed to be bolded? (so it's random vector?)
\end{itemize}
\begin{example} Bayes 1 (Binomial Bayes)

Let $X_1,\ldots,X_n$ be i.i.d. Bernoulli($p$). Then
$$\sum X_i \sim Binomial(n,p)$$
Assume that prior of the $p$ is the $Beta(\alpha, \beta)$ distribution with some
values of $\alpha$ and $\beta$, which are set before the data are seen. The
joint pdf of $(y,p)$ is
$$f(y,p) = \underbrace{f(y | p)}_{Binomial} \underbrace{\pi(p)}_{Beta} =
\left[{n\choose
y}p^y(1-p)^{n-p}\right]\left[\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{\alpha-1}(1-p)^{\beta-1}\right]$$

$$={n \choose y}
\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{y+\alpha-1}(1-p)^{n-y+\beta-1}$$
The marginal pdf of $Y$ is
$$f(y) = \int_{0}^{1}f(y,p)\ud p = {n \choose
p}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \frac{\Gamma(y
+ \alpha)\Gamma(n-y+\beta)}{\Gamma(n+\alpha+\beta)}$$
which is called the beta-binomial pdf. The posterior distribution is
$$f(p | y) = \frac{f(y,p)}{f(y)}=\frac{\Gamma(n+\alpha+\beta)}{\Gamma(y +
\alpha)\Gamma(n-y+\beta)}p^{y+\alpha-1}(1-p)^{n-y+\beta-1}$$
which is the Beta($y+\alpha$,$n-y+\beta$).

A natural point estimator of $p$ is the expected value $E(p | \bm x)$ of
posterior distribution:
$$\hat p_\beta = \frac{y-\alpha}{\alpha+\beta+n}$$
It is a weighted mean of the prior mean $\frac{\alpha}{\alpha+\beta}$ and the
ML-estimator $\frac{y}{n}$:
$$\frac{n}{\alpha+\beta+n}\frac{y}{n}+\frac{\alpha+\beta}{\alpha+\beta+n}\frac{\alpha}{\alpha+\beta}
= w \frac{y}{n}+(1-w)\frac{\alpha}{\alpha+\beta}, \text{ where }
w=\frac{n}{\alpha+\beta+n} \in [0,1]$$
\textbf{Note} The Beta distribution is the conjugate prior for the Binomial
distribution, because the posterior distribution belongs to the same family. The
use of conjugate priors leads to mathematically beautiful results, but are
not otherwise justified by any theory.
\end{example}
\begin{itemize}
  \item In the conditional distribution
  $$\pi(\bm \theta | \bm x) = \frac{f(\bm x | \bm \theta)\pi(\theta)}{m(\bm
  x)}$$
  the denominator $m(\bm x) = \int f(\bm x | \bm \theta) \pi(\bm \theta) \ud
  \theta$ is hard to compute in general case.
  \begin{itemize}
    \item The modern computational methods, so called Markov chain Monte Carlo
    (MCMC) methods allow simulation of observations from $\pi(\bm \theta | \bm
    x)$ without the need to evaluate $m(\bm x)$.
  \end{itemize}
  %TOOD: Left out some of the stuff in the begining of notes 12 page 1 since
  % recap. Not needed?
  \item The most simple MCMC method is the Metropolis algorithm. (Denote
  $f(x|\theta)\pi(\theta)$ by $a(\theta)$)
  \begin{itemize}
    \item[1.] Choose an arbitrary starting point $\theta_0$ form $\Theta$ (the
    sample space of $\theta$). Choose an arbitrary symmetric pdf (the proposal
    density) $g(\theta_{t+1} | \theta_t)$, which suggests the new value of
    $\theta$ ($\theta_{t+1}$) given the previous value $\theta_t$. Usually we
    use $N(\theta_t | \sigma^2)$ as the proposal density.
    \item[2.] At each iteration t
    \begin{itemize}
      \item[2.1] Generate a candidate of $\theta$: $\theta'$ using
      $g(\theta_{t+1}| \theta_t)$.
      \item[2.2] Compute the acceptance ratio
      $$\alpha = \frac{a(\theta')}{a(\theta_t)}$$
      \item[2.3] If $\alpha < 1$, then accept $\theta'$ as $\theta_{t+1}$ with
      probability $alpha$.
      %TODO: Left out a plot in notes 11 page 1 end. Is it needed?
    \end{itemize}
    Repeat until a desired sample size has been obtained.
  \end{itemize}
\end{itemize}
\begin{example} Bayes 2

Consider a sample $\bm x' = \begin{bmatrix} 9.76 & 9.08 & 16.23 & 10.28 & 10.51
\end{bmatrix}$. The aim is to estimate the population mean $\theta$. We use
restrictive prior for it:
$$\pi(\theta) \sim Uniform(7, 13)$$
In R-script \verb#Bayes.R# there is function postProfile that implements
function $a(\theta)$. $N(\theta_t, 1)$ is used as $g(\theta_{t+1} | \theta_t )$.
We generate a sample size of $n=10000$ and use the posterior sample mean as the
point estimate of $\theta$.
\end{example}
\textbf{Notes}
\begin{itemize}
  \item MCMC methods require only that $\pi(\theta)f(\bm x | \theta)$ can be
  evaluated.
  \item The sample generated from the posterior is not independent: the sequence
  $\theta_1,\ldots, \theta_t,\ldots,\theta_N$ has autocorrelation so that
  successive values are dependent.
  \item Commonly used MCMC methods are the Metropolis-Hastings-algorithm and the
  Gibbs sampler.
\end{itemize}
\section{Methods for evaluating estimators}
\begin{definition}
The mean squared error (MSE) of an estimator $W$ of parameter $\theta$ is the
function of $\theta$ defined by
$$E_\theta(W-\theta)^2$$
\end{definition}
\begin{itemize}
  \item In general, any increasing function of the aboslute distancec
  $\abs{W-\theta}$ is a reasonable measure for the quality of $W$ (e.g. mean
  absolute error $E_\theta(W-\theta)$), but MSE is 1) mathematically factorable
  and 2) has the interpretation
  $$E_\theta(W-\theta)^2 = Var_\theta(W)+(E_\theta(W-\theta))^2$$
  $$MSE = variance + bias^2$$
\end{itemize}
\begin{definition}
The bias of the point estimator $W$ is the difference
$$Bias_\theta(W) = E_\theta(W) - \theta$$
\end{definition}
\begin{itemize}
  \item Estimator $W$ is said to be unbiased if $Bias_\theta(W) = 0$ for all
  $\theta$
  \item MSE as a criterion for an estimator leads to estimators that have small
  bias and variance.
\end{itemize}
\begin{example} (Normal MSE)

Let $X_1,\ldots,X_n$ be i.i.d. N($\mu, \sigma^2$). The statistics $\bar x$ and
$s^2$ are unbiased estimators of $\mu$ and $\sigma^2$ since
$$E(\bar x) - \mu = \mu - \mu = 0$$
$$E(s^2) = \sigma^2 \to E(s^2) - \sigma^2 = 0$$
\textbf{Note} These results are true even if the population is not normally
distributed.

The MSE of these estimators are

$$(*)~MSE(\bar X) = E(\bar X - \mu)^2 = Var(\bar X) = \frac{\sigma^2}{n}$$
$$(**)~MSE(s^2) = E(s^2 - \sigma^2)^2 = Var(s^2)= \ldots =
\frac{2\sigma^4}{n-1}$$
Note that (*) did not require normality, but (**) did. An alternative estimator
for $\sigma^2$ is the MLE/moment estimator
$$\hat \sigma^2 = \frac{1}{n}(X_i - \bar X)^2 = \frac{n-1}{n}s^2$$
$$E(\hat \sigma^2) = E\left(\frac{n-1}{n}s^2\right) = \frac{n-1}{n}E(s^2) =
\frac{n-1}{n}\sigma^2$$
Now $E(\hat \sigma^2)- \sigma^2 = \frac{n-1}{n}\sigma^2 - \sigma^2 \neq 0$,
meaning $\hat \sigma^2$ is a unbiased estimator of $\sigma^2$.

The variance of $\hat\sigma^2$ is
$$Var(\hat\sigma^2) = Var(\frac{n-1}{n}s^2) = \left(\frac{n-1}{n}\right)^2
Var(s^2) = \frac{2(n-1)}{n^2}\sigma^4$$
The MSE of $\hat \sigma^2$ is
$$E(\hat \sigma^2 - \sigma^2)^2 = variance + bias^2 = \frac{2(n-1)}{n^2}\sigma^4
+ \left(\frac{n-1}{n}\sigma^2 - \sigma^2 \right)^2 = \frac{2n-1}{n^2}\sigma^2$$
$$\underbrace{E(\hat \sigma^2 - \sigma^2)^2}_{MSE(\hat\sigma^2)} =
\frac{2n-1}{n^2}\sigma^2 < \frac{2\sigma^4}{n-1} =
\underbrace{E(s^2-\sigma^2)^2}_{MSE(s^2)}$$
$$\frac{2n-1}{n^2} = \frac{2-\frac{1}{n}}{n} < \frac{2}{n-1}$$
$\to$ Allowing bias in the estimator of $\sigma^2$ leads to gain in terms of
MSE.
\begin{itemize}
  \item The bias is negative $\to$ ML underestimates the variance on average.
  \item In general, MSE is justified choice to evaluate estimators of location
  parameters, but not the scale parameters.
\end{itemize}
\end{example}
It is often impossible to find a single estimator that is best in terms of MSE.
In addition, the bias of an estimator is often seen as a severe problem, or
inconvenience. Therefore, we often restrict consideration in the class of
unbiased estimatros. In that class
$$E_\theta(E-\theta)^2 = Var_\theta(W)$$
Therefore, form among unbiased estimators, we should select the one with the
lowest variance.
\begin{definition}
An estimator $W^*$ is the best unbiased estimator (BUE) of $\tau(\theta)$, if it
satisfies $E_\theta(W^*)=\tau(\theta)$ for all $\theta$ and for any other
estimator $W$ with $E_\theta(W)=\tau(\theta)$ we have
$$Var_\theta(\tau^*) \leq Var_\theta(W) \text{ for all } \theta$$
$W^*$ is also called uniform minimum variance unbiased estimator of
$\tau(\theta)$ (UMVUE).
\end{definition}
\begin{example}
Let $X_1,\ldots,X_n$ be i.i.d. Poisson($\lambda$) and let $\bar X$ and $s^2$ be
the sample mean and sample variance respectively. Recall that in
Poisson($\lambda$) population $E(X) = Var(X) = \lambda$. Therefore, both $\bar
X$ and $s^2$ are unbiased estimators of $\lambda$. Which one to select?
$$Var_\lambda(\bar X) = \frac{\lambda}{n}$$
$$Var_\lambda(s^2) = \ldots \text{ long calculation }$$
Conclution however will be, that $Var_\lambda(\bar X) \leq Var_\lambda(s^2)$ for
all $\lambda$. So, from among these two candidates, $\bar X$ should be selected.
But is $\bar X$ really the best?

We could construct infintely many unbiased estimators as weighted mean of $\bar
X$ and $s^2$:
$$W_W = w \bar X + (1-w)s^2 \text{ where } w \in [0,1]$$
Could some selection of w lead to smaller variance than $Var(\bar X) =
\frac{\lambda}{n}$? Or is there some other estimator that has $Var(w) =
\frac{\lambda}{n}$?
%TODO: Left out the $\hat \lambda = \bar X$ at notes 12 page 8 end. Is it
% needed?
\end{example}
\end{document}
