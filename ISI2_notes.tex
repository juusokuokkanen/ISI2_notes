%%%%%%%%  Document class  %%%%%%%%%%%%
\documentclass[10pt, twoside, a4paper]{book}

%%%%%%%%  Packages   %%%%%%%%%%%%%%%%
\usepackage{natbib}
%\usepackage{d:/laurim/biometria/lshort/src/lshort}
%\usepackage{numline}
%\usepackage{lineno}
\bibpunct{(}{)}{,}{a}{}{,}
\usepackage{amsmath,amssymb}
\usepackage{bm} 
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{color}

%\newenvironment{Rcode}[1]%
%{\scriptsize{\ttfamily{\bf The R-code for #1}} \\
%\ttfamily \noindent\ignorespaces }
%{\par\noindent%
%  \ignorespacesafterend}
  
\newenvironment{Rcode}[1]%
{\ttfamily{\bf The R-code for #1}
\scriptsize \\\par\verbatim}
{\endverbatim\par} 

\newenvironment{Rcode2}%
{\scriptsize \par\verbatim}%
{\endverbatim\par}

\usepackage{graphicx}
%\usepackage{subfigure}
%\usepackage[nolists]{endfloat}
\usepackage{amsthm}

\usepackage{times}
%\usepackage[T1]{fontenc}

%\usepackage{breqn} % Helps with multline \left \right problems.

\newcommand{\foldPath}{c:/laurim/tex/}

\newcommand{\beq}{\begin{equation*}}
\newcommand{\eeq}{\end{equation*}}
\newcommand{\ud}{\,\mathrm{d}}
\newcommand{\ureal}{\,\mathbb{R}}
\newcommand{\uninf}{\,-\infty}
\newcommand{\uinf}{\,-\infty}
\usepackage{graphicx}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\corr}{corr}
\DeclareMathOperator{\sd}{sd}

\theoremstyle{definition}

\newtheorem{example}{Example}[chapter]
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
%\bibpunct{(}{)}{,}{a}{}{,}
\bibliographystyle{\foldPath saf}
%\bibliographystyle{alpha}
%\bibliographystyle{\foldPath wiley}
%\bibliographystyle{unsrt}

%%%%%%%%  Page Setup %%%%%%%%%%%%%%%%%%
%\topmargin      0pt
%\headheight     0pt 
%\headsep        0pt 
%\textheight   648pt 
%\oddsidemargin  0pt
%\textwidth    468pt 

%%%%%%%% Document %%%%%%%%%%%%%%%%%%%
\begin{document}
\pagenumbering{Roman}
\setlength{\baselineskip}{16pt}

%% Front matter
%\title{Forest biometrics with examples in R}
\title{Introduction to statistical inference 2}

\author{Lauri Meht\"atalo\\
	   University of Eastern Finland \\ School of Computing
	   }

\date{\normalsize \today}
\maketitle
\tableofcontents

%\section*{Preface}

%These are lecture notes for course Biometrics: R-statistics in Forest Sciences. In writing these notes, I have used several sources. In writing chapter 1, the main sources have been the second edition of the classical book ``Statistical Inference'' by George Casella and Roger L Berger\citep{Casellaandberger2002} and ``Methods for Forest Biometrics'' by Juha Lappi  \citep{Lappi1993}. In addition, I have used lecture notes of courses on mathematical statistics (by Eero Korpelainen) and statistical inference (by Jukka Nyblom). For chapters 2 and 3, the main sources have been the lecture notes of Jukka Nyblom on Regression analysis and Linear models, the book of Jose Pinheiro and Douglas Bates on Linear models with R \citep{Pinheiroandbates2000}, and the book ``Generalized, Linear and Mixed Models'' by CharlesMcCulloch and Shayle R Searle \citep{McCullochandsearle2001}. For the GLMs and GLMMs, I have used \citet{McCullochandsearle2001}, too. The last chapter on models systems is based mainly on book ``Econometric analysis'' by William H Greene \citep{Greene1997}. In addition, good sources of information have been the lecture notes of Annika Kangas on Forest Biometrics, the books of of Julian Faraway \citep{Faraway2004, Faraway2006}, Keith E Muller and Paul W Stewart \citep{Mullerandstewart2006}, and William N. Venables and Brian D. Ripley \citep{Venablesandripley2002}.   

\mainmatter
\pagenumbering{arabic}

\chapter {Recap from ``Introduction to statistical inference 1''}
\section{Random variable}
\begin{itemize}
\item Random variable is a function from sample space $\mathcal{S}$ of an
experiment to sample space of the random variable $\mathcal{X}$, which is set of real numbers.
$$X: \mathcal{S} \rightarrow \mathcal{X}$$
\item The sample space is a set of all possible values random variable can get.
\item $\mathcal{X}$ can be
\begin{itemize}
  \item an interval of real axis (continuous random variable).
  $$\mathcal{X} = [0, 10),~ \mathcal{X} = [0, 10],~ \mathcal{X} = (0, 10)$$
  \item An uncountable set of integers (discrete random variable) 
  $$\mathcal{X} = \{0,1,2,\ldots\}$$
  \item A countable set of integers or real numbers (discrete random variable)
  $$\mathcal{X} =\{0,1\},~\mathcal{X} = \{0,0.5,1\},~ \mathcal{X} =
  \{0,1,\ldots,10\}$$ 
\end{itemize}
\item Probabilities associated with each value of $X$ are defined by the
cumulative distribution function (cdf for short).
$$F_X(x) = P(X \leq x),~\text{where } -\infty < x < \infty$$
\item[\bf{Note:}]$F_X(x)$ is a step function if $X$ is discrete.
\item[\bf{Note:}]$F_X(x)$ is a continuous function if $X$ is continuous.
%TODO: Two plots visualizing discreate and continous CDFs. Notes 1, page 2.
\item $F_X(x)$ or $F(x)$ is cdf, if
\begin{itemize}
  \item [1)] $\lim_{x \to -\infty} F(x) = 0$ and $\lim_{x \to
  \infty} F(x) = 1$
  \item [2)] $F(x)$ is non-decreasing
  \item [3)] $F(x)$ is right-continuous
\end{itemize}
\item Cdf is useful in calculation of any probabilities; for example
$$P(a < X \leq b) = F(b) - F(a)$$
\item [\bf{Note:}] Be careful with $<$ and $\leq$ when working with discreate
random variables.
\item The probability density function (pdf for short) is defined for
continuous random variable as
$$f_X(x) = F'(x) = \frac{\ud F_X(x)}{\ud x}, ~ -\infty < x < \infty$$
and
$$\int_{-\infty}^x f_X(t)\ud t = F_X(x)$$
\item The probability mass function (pmf for short) is defined for discrete
random variables as
$$f_X(x) = P(X=x)$$
$$F_X(x) = \sum_{k=1}^x f_X(k)$$
\end{itemize}
\section{Transformations of random variable}
\begin{itemize}
  \item Consider a monotonic function $g: \mathcal{X} \to \mathcal{Y}$
  \item $Y = g(X)$ is also a random variable; function $g$ is called an
  transformation (muunnos).
  \item If $g(x)$ is a increasing function of $x$, then
  $$F_Y(y) = F_X(g^{-1}(y))$$
  \item If $g(x)$ is a decreasing function of $x$, then
  $$F_Y(y) = 1 - F_X(g^{-1}(y))$$
  \item The pdf of continuous $Y$ is
  $$f_Y(y) = F_Y'(y)$$
\end{itemize}
\section{Expected values}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
E(X) = \mu_X = \left\{
\begin{array}{ll}
\int_{-\infty}^\infty x f(x) \ud x & \text{if $X$ is continuous} \\
\sum_{x \in \mathcal{X}} x f(x) & \text{if $X$ is discrete}
\end{array} \right.
\end{equation*}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
E(g(X)) =  \left\{
\begin{array}{ll}
\int_{-\infty}^\infty g(x) f(x) \ud x & \text{if $X$ is continuous} \\
\sum_{x \in \mathcal{X}} g(x) f(x) & \text{if $X$ is discrete}
\end{array} \right.
\end{equation*}
\section{Variance}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
\sigma_X^2 = Var(X) & = E(X - \mu_X)^2 \\
& = E(X^2 - 2X\mu_X + \mu_X^2) \\
& = E(X^2) - E(2X\mu_X) + E(\mu_X^2) \\
& = E(X^2) - \underbrace{2\mu_X \underbrace{E(X)}_{\mu_X}}_{2\mu_X^2} +
E(\mu_X^2) \\
& = E(X^2) - \mu_X^2 \\
\end{array}
\end{equation*}

\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
sd(X) = \sqrt{Var(X)} = \sigma_X
\end{array}
\end{equation*}

\section{Bivariate random variables}
\begin{itemize}
  \item For two discrete random variables, the joint pmf is defined as
  $$f_{X,Y}(x,y)=P(X=x, Y=y)$$
  \item For two continuous random variables, we define the joint pdf
  $f_{X,Y}(x,y)$ as
  $$P((X,Y) \in A) = \iint\limits_A f(x,y) \ud x \ud y$$
  \item The expected value for transformation
$g(X,Y) : \ureal^2 \to \ureal$ (for example, \\ $g(X,Y) = XY$ or
$g(X,Y) =
\frac{X}{Y}$) is
$$E(g(X,Y)) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y)f(x,y) \ud
x \ud y$$
if $(X,Y)$ is continuous, and
$$E(g(X,Y)) = \sum_{x,y \in \ureal^2} g(x,y)f(x,y)$$
if $(X,Y)$ is discrete.
\item The marginal pmf / pdf for $X$ are
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
f_X(x) = \sum_{y \in \ureal} f_{X,Y}(x,y) & \text{(pmf)} \\
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \ud y & \text{(pdf)} \\
\end{array}
\end{equation*}
and correspondingly for $Y$.
\item The conditional pmf / pdf are both defined as
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
f(y \mid x)  = \frac{f_{X,Y}(x,y)}{f_X(x)} & \text{(for both doscrete and
continuous random variables)}
\end{array}
\end{equation*}
and correspondingly for $x \mid y$.
\end{itemize}
\section{Independence}
\begin{itemize}
  \item Random variables are said to be independent ($X \indep Y$) if
  $$f_{X,Y}(x,y) = f_X(x)f_Y(y)$$
  \item For independent random variables, conditional distribution $y \mid x$ is 
  $$f(y \mid x) = f_y(y)$$
  regardless of the value of $x$.
\end{itemize}

\section{Covariance}
\begin{itemize}
  \item Covariance measures the linear association between two random variables
  $X$ and $Y$
  \begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{lll}
	cov(X,Y) & = E((X-\mu_X)(Y-\mu_Y)) \\
	& = E(XY) - \mu_X\mu_Y \\
	cov(X,Y) & = cov(Y,X) \\
	corr(X,Y) & = \rho_{XY} = \frac{cov(X,Y)}{\sigma_X \sigma_Y} & \text{where } -1
	\leq \rho_{XY} \leq 1
	\end{array}
	\end{equation*}
	\item[\bf{Note:}] $cov(X,X) = Var(X)$
	%TODO: Small plot left out. Notes 1, page 8.
	\item[\bf{Note:}] If $X \indep Y$, then $cov(X,Y) = 0$, but if $cov(X,Y) = 0$,
	it does not mean $X$ and $Y$ are necessarily independent.
\end{itemize}

\end{document}
